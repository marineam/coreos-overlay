diff --git a/bin/ebuild b/bin/ebuild
index f8b6d79..66ea6cf 100755
--- a/bin/ebuild
+++ b/bin/ebuild
@@ -204,8 +204,9 @@ def discard_digests(myebuild, mysettings, mydbapi):
 		portage._doebuild_manifest_exempt_depend += 1
 		pkgdir = os.path.dirname(myebuild)
 		fetchlist_dict = portage.FetchlistDict(pkgdir, mysettings, mydbapi)
-		from portage.manifest import Manifest
-		mf = Manifest(pkgdir, mysettings["DISTDIR"],
+		mf = mysettings.repositories.get_repo_for_location(
+			os.path.dirname(os.path.dirname(pkgdir)))
+		mf = mf.load_manifest(pkgdir, mysettings["DISTDIR"],
 			fetchlist_dict=fetchlist_dict, manifest1_compat=False)
 		mf.create(requiredDistfiles=None,
 			assumeDistHashesSometimes=True, assumeDistHashesAlways=True)
@@ -228,10 +229,8 @@ build_dir_phases = set(["setup", "unpack", "prepare", "configure", "compile",
 # sourced again even if $T/environment already exists.
 ebuild_changed = False
 if mytree == "porttree" and build_dir_phases.intersection(pargs):
-	metadata, st, emtime = \
-		portage.portdb._pull_valid_cache(cpv, ebuild, ebuild_portdir)
-	if metadata is None:
-		ebuild_changed = True
+	ebuild_changed = \
+		portage.portdb._pull_valid_cache(cpv, ebuild, ebuild_portdir)[0] is None
 
 tmpsettings = portage.config(clone=portage.settings)
 tmpsettings["PORTAGE_VERBOSE"] = "1"
diff --git a/bin/ebuild-helpers/prepstrip b/bin/ebuild-helpers/prepstrip
index d25259d..098e0c6 100755
--- a/bin/ebuild-helpers/prepstrip
+++ b/bin/ebuild-helpers/prepstrip
@@ -4,31 +4,60 @@
 
 source "${PORTAGE_BIN_PATH:-/usr/lib/portage/bin}"/isolated-functions.sh
 
+# avoid multiple calls to `has`.  this creates things like:
+#   FEATURES_foo=false
+# if "foo" is not in $FEATURES
+tf() { "$@" && echo true || echo false ; }
+exp_tf() {
+	local flag var=$1
+	shift
+	for flag in "$@" ; do
+		eval ${var}_${flag}=$(tf has ${flag} ${!var})
+	done
+}
+exp_tf FEATURES installsources nostrip splitdebug
+exp_tf RESTRICT binchecks installsources strip
+
 banner=false
 SKIP_STRIP=false
-if has nostrip ${FEATURES} || \
-   has strip ${RESTRICT}
-then
+if ${RESTRICT_strip} || ${FEATURES_nostrip} ; then
 	SKIP_STRIP=true
 	banner=true
-	has installsources ${FEATURES} || exit 0
+	${FEATURES_installsources} || exit 0
 fi
 
-STRIP=${STRIP:-${CHOST}-strip}
-type -P -- ${STRIP} > /dev/null || STRIP=strip
-OBJCOPY=${OBJCOPY:-${CHOST}-objcopy}
-type -P -- ${OBJCOPY} > /dev/null || OBJCOPY=objcopy
+# look up the tools we might be using
+for t in STRIP:strip OBJCOPY:objcopy READELF:readelf ; do
+	v=${t%:*} # STRIP
+	t=${t#*:} # strip
+	eval ${v}=\"${!v:-${CHOST}-${t}}\"
+	type -P -- ${!v} >/dev/null || eval ${v}=${t}
+done
+
+# Figure out what tool set we're using to strip stuff
+unset SAFE_STRIP_FLAGS DEF_STRIP_FLAGS SPLIT_STRIP_FLAGS
+case $(${STRIP} --version 2>/dev/null) in
+*elfutils*) # dev-libs/elfutils
+	# elfutils default behavior is always safe, so don't need to specify
+	# any flags at all
+	SAFE_STRIP_FLAGS=""
+	DEF_STRIP_FLAGS="--remove-comment"
+	SPLIT_STRIP_FLAGS="-f"
+	;;
+*GNU*) # sys-devel/binutils
+	# We'll leave out -R .note for now until we can check out the relevance
+	# of the section when it has the ALLOC flag set on it ...
+	SAFE_STRIP_FLAGS="--strip-unneeded"
+	DEF_STRIP_FLAGS="-R .comment -R .GCC.command.line"
+	SPLIT_STRIP_FLAGS=
+	;;
+esac
+: ${PORTAGE_STRIP_FLAGS=${SAFE_STRIP_FLAGS} ${DEF_STRIP_FLAGS}}
 
-# We'll leave out -R .note for now until we can check out the relevance
-# of the section when it has the ALLOC flag set on it ...
-export SAFE_STRIP_FLAGS="--strip-unneeded"
-export PORTAGE_STRIP_FLAGS=${PORTAGE_STRIP_FLAGS-${SAFE_STRIP_FLAGS} -R .comment}
 prepstrip_sources_dir=/usr/src/debug/${CATEGORY}/${PF}
 
-if has installsources ${FEATURES} && ! type -P debugedit >/dev/null ; then
-	ewarn "FEATURES=installsources is enabled but the debugedit binary could not"
-	ewarn "be found. This feature will not work unless debugedit is installed!"
-fi
+type -P debugedit >/dev/null && debugedit_found=true || debugedit_found=false
+debugedit_warned=false
 
 unset ${!INODE_*}
 
@@ -41,19 +70,33 @@ inode_var_name() {
 }
 
 save_elf_sources() {
-	has installsources ${FEATURES} || return 0
-	has installsources ${RESTRICT} && return 0
-	type -P debugedit >/dev/null || return 0
+	${FEATURES_installsources} || return 0
+	${RESTRICT_installsources} && return 0
+	if ! ${debugedit_found} ; then
+		if ! ${debugedit_warned} ; then
+			debugedit_warned=true
+			ewarn "FEATURES=installsources is enabled but the debugedit binary could not"
+			ewarn "be found. This feature will not work unless debugedit is installed!"
+		fi
+		return 0
+	fi
 
 	local x=$1
 	local inode=$(inode_var_name "$x")
 	[[ -n ${!inode} ]] && return 0
-	debugedit -b "${WORKDIR}" -d "${prepstrip_sources_dir}" \
-		-l "${T}"/debug.sources "${x}"
+
+	# since we're editing the ELF here, we should recompute the build-id
+	# (the -i flag below).  save that output so we don't need to recompute
+	# it later on in the save_elf_debug step.
+	buildid=$(debugedit -i \
+		-b "${WORKDIR}" \
+		-d "${prepstrip_sources_dir}" \
+		-l "${T}"/debug.sources \
+		"${x}")
 }
 
 save_elf_debug() {
-	has splitdebug ${FEATURES} || return 0
+	${FEATURES_splitdebug} || return 0
 
 	local x=$1
 	local y="${D}usr/lib/debug/${x:${#D}}.debug"
@@ -61,23 +104,30 @@ save_elf_debug() {
 	# dont save debug info twice
 	[[ ${x} == *".debug" ]] && return 0
 
-	# this will recompute the build-id, but for now that's ok
-	local buildid="$( type -P debugedit >/dev/null && debugedit -i "${x}" )"
-
-	mkdir -p $(dirname "${y}")
+	mkdir -p "${y%/*}"
 
 	local inode=$(inode_var_name "$x")
 	if [[ -n ${!inode} ]] ; then
 		ln "${D}usr/lib/debug/${!inode:${#D}}.debug" "$y"
 	else
 		eval $inode=\$x
-		${OBJCOPY} --only-keep-debug "${x}" "${y}"
-		${OBJCOPY} --add-gnu-debuglink="${y}" "${x}"
-		[[ -g ${x} ]] && chmod go-r "${y}"
-		[[ -u ${x} ]] && chmod go-r "${y}"
-		chmod a-x,o-w "${y}"
+		if [[ -e ${T}/prepstrip.split.debug ]] ; then
+			mv "${T}"/prepstrip.split.debug "${y}"
+		else
+			${OBJCOPY} --only-keep-debug "${x}" "${y}"
+			${OBJCOPY} --add-gnu-debuglink="${y}" "${x}"
+		fi
+		local args="a-x,o-w"
+		[[ -g ${x} || -u ${x} ]] && args+=",go-r"
+		chmod ${args} "${y}"
 	fi
 
+	# if we don't already have build-id from debugedit, look it up
+	if [[ -z ${buildid} ]] ; then
+		# convert the readelf output to something useful
+		buildid=$(${READELF} -x .note.gnu.build-id "${x}" 2>/dev/null \
+			| awk '$NF ~ /GNU/ { getline; printf $2$3$4$5; getline; print $2 }')
+	fi
 	if [[ -n ${buildid} ]] ; then
 		local buildid_dir="${D}usr/lib/debug/.build-id/${buildid:0:2}"
 		local buildid_file="${buildid_dir}/${buildid:2}"
@@ -87,11 +137,31 @@ save_elf_debug() {
 	fi
 }
 
+process_elf() {
+	local x=$1 strip_flags=${*:2}
+
+	vecho "   ${x:${#D}}"
+	save_elf_sources "${x}"
+
+	if ${strip_this} ; then
+		# see if we can split & strip at the same time
+		if [[ -n ${SPLIT_STRIP_FLAGS} ]] ; then
+			${STRIP} ${strip_flags} \
+				-f "${T}"/prepstrip.split.debug \
+				-F "${x##*/}.debug" \
+				"${x}"
+			save_elf_debug "${x}"
+		else
+			save_elf_debug "${x}"
+			${STRIP} ${strip_flags} "${x}"
+		fi
+	fi
+}
+
 # The existance of the section .symtab tells us that a binary is stripped.
 # We want to log already stripped binaries, as this may be a QA violation.
 # They prevent us from getting the splitdebug data.
-if ! has binchecks ${RESTRICT} && \
-	! has strip ${RESTRICT} ; then
+if ! ${RESTRICT_binchecks} && ! ${RESTRICT_strip} ; then
 	log=$T/scanelf-already-stripped.log
 	qa_var="QA_PRESTRIPPED_${ARCH/-/_}"
 	[[ -n ${!qa_var} ]] && QA_PRESTRIPPED="${!qa_var}"
@@ -149,6 +219,7 @@ do
 	# actually causes problems.  install sources for all
 	# elf types though cause that stuff is good.
 
+	buildid=
 	if [[ ${f} == *"current ar archive"* ]] ; then
 		vecho "   ${x:${#D}}"
 		if ${strip_this} ; then
@@ -156,12 +227,7 @@ do
 			${STRIP} -g "${x}"
 		fi
 	elif [[ ${f} == *"SB executable"* || ${f} == *"SB shared object"* ]] ; then
-		vecho "   ${x:${#D}}"
-		save_elf_sources "${x}"
-		if ${strip_this} ; then
-			save_elf_debug "${x}"
-			${STRIP} ${PORTAGE_STRIP_FLAGS} "${x}"
-		fi
+		process_elf "${x}" ${PORTAGE_STRIP_FLAGS}
 	elif [[ ${f} == *"SB relocatable"* ]] ; then
 		vecho "   ${x:${#D}}"
 		save_elf_sources "${x}"
@@ -173,9 +239,9 @@ do
 done
 
 if [[ -s ${T}/debug.sources ]] && \
-        has installsources ${FEATURES} && \
-        ! has installsources ${RESTRICT} && \
-        type -P debugedit >/dev/null
+   ${FEATURES_installsources} && \
+   ! ${RESTRICT_installsources} && \
+   ${debugedit_found}
 then
 	vecho "installsources: rsyncing source files"
 	[[ -d ${D}${prepstrip_sources_dir} ]] || mkdir -p "${D}${prepstrip_sources_dir}"
diff --git a/bin/egencache b/bin/egencache
index 1b4265d..5e68980 100755
--- a/bin/egencache
+++ b/bin/egencache
@@ -38,6 +38,7 @@ except ImportError:
 from portage import os, _encodings, _unicode_encode, _unicode_decode
 from _emerge.MetadataRegen import MetadataRegen
 from portage.cache.cache_errors import CacheError, StatCollision
+from portage.cache import metadata
 from portage.manifest import guessManifestFileType
 from portage.util import cmp_sort_key, writemsg_level
 from portage import cpv_getkey
@@ -203,9 +204,11 @@ class GenCache(object):
 			consumer=self._metadata_callback,
 			max_jobs=max_jobs, max_load=max_load)
 		self.returncode = os.EX_OK
-		metadbmodule = portdb.settings.load_best_module("portdbapi.metadbmodule")
-		self._trg_cache = metadbmodule(portdb.porttrees[0],
-			"metadata/cache", portage.auxdbkeys[:])
+		conf = portdb.repositories.get_repo_for_location(portdb.porttrees[0])
+		self._trg_cache = conf.get_pregenerated_cache(portage.auxdbkeys[:],
+			force=True, readonly=False)
+		if self._trg_cache is None:
+			raise Exception("cache format %s isn't supported" % (conf.cache_format,))
 		if rsync:
 			self._trg_cache.raise_stat_collision = True
 		try:
@@ -215,13 +218,15 @@ class GenCache(object):
 			pass
 		self._existing_nodes = set()
 
-	def _metadata_callback(self, cpv, ebuild_path, repo_path, metadata):
+	def _metadata_callback(self, cpv, repo_path, metadata, ebuild_hash):
 		self._existing_nodes.add(cpv)
 		self._cp_missing.discard(cpv_getkey(cpv))
 		if metadata is not None:
 			if metadata.get('EAPI') == '0':
 				del metadata['EAPI']
 			try:
+				chf = self._trg_cache.validation_chf
+				metadata['_%s_' % chf] = getattr(ebuild_hash, chf)
 				try:
 					self._trg_cache[cpv] = metadata
 				except StatCollision as sc:
@@ -240,7 +245,7 @@ class GenCache(object):
 						max_mtime += 1
 					max_mtime = long(max_mtime)
 					try:
-						os.utime(ebuild_path, (max_mtime, max_mtime))
+						os.utime(ebuild_hash.location, (max_mtime, max_mtime))
 					except OSError as e:
 						self.returncode |= 1
 						writemsg_level(
diff --git a/bin/misc-functions.sh b/bin/misc-functions.sh
index 8c191ff..05136f4 100755
--- a/bin/misc-functions.sh
+++ b/bin/misc-functions.sh
@@ -882,8 +882,16 @@ preinst_selinux_labels() {
 dyn_package() {
 	# Make sure $PWD is not ${D} so that we don't leave gmon.out files
 	# in there in case any tools were built with -pg in CFLAGS.
+
 	cd "${T}"
-	install_mask "${PORTAGE_BUILDDIR}/image" "${PKG_INSTALL_MASK}"
+
+	local PROOT="${T}/packaging"
+	# make a temporary copy of ${D} so that any modifications we do that
+	# are binpkg specific, do not influence the actual installed image.
+	cp -la "${PORTAGE_BUILDDIR}/image" "${PROOT}" || die "failed creating packaging tree"
+
+	install_mask "${PROOT}" "${PKG_INSTALL_MASK}"
+
 	local tar_options=""
 	[[ $PORTAGE_VERBOSE = 1 ]] && tar_options+=" -v"
 	# Sandbox is disabled in case the user wants to use a symlink
@@ -892,7 +900,7 @@ dyn_package() {
 	[ -z "${PORTAGE_BINPKG_TMPFILE}" ] && \
 		die "PORTAGE_BINPKG_TMPFILE is unset"
 	mkdir -p "${PORTAGE_BINPKG_TMPFILE%/*}" || die "mkdir failed"
-	tar $tar_options -cf - $PORTAGE_BINPKG_TAR_OPTS -C "${D}" . | \
+	tar $tar_options -cf - $PORTAGE_BINPKG_TAR_OPTS -C "${PROOT}" . | \
 		$PORTAGE_BZIP2_COMMAND -c > "$PORTAGE_BINPKG_TMPFILE"
 	assert "failed to pack binary package: '$PORTAGE_BINPKG_TMPFILE'"
 	PYTHONPATH=${PORTAGE_PYM_PATH}${PYTHONPATH:+:}${PYTHONPATH} \
@@ -913,6 +921,9 @@ dyn_package() {
 	[ -n "${md5_hash}" ] && \
 		echo ${md5_hash} > "${PORTAGE_BUILDDIR}"/build-info/BINPKGMD5
 	vecho ">>> Done."
+
+	# cleanup our temp tree
+	rm -rf "${PROOT}"
 	cd "${PORTAGE_BUILDDIR}"
 	>> "$PORTAGE_BUILDDIR/.packaged" || \
 		die "Failed to create $PORTAGE_BUILDDIR/.packaged"
@@ -985,6 +996,21 @@ success_hooks() {
 	done
 }
 
+install_hooks() {
+	local hooks_dir="${PORTAGE_CONFIGROOT}etc/portage/hooks/install"
+	local fp
+	local ret=0
+	shopt -s nullglob
+	for fp in "${hooks_dir}"/*; do
+		if [ -x "$fp" ]; then
+			"$fp"
+			ret=$(( $ret | $? ))
+		fi
+	done
+	shopt -u nullglob
+	return $ret
+}
+
 if [ -n "${MISC_FUNCTIONS_ARGS}" ]; then
 	source_all_bashrcs
 	[ "$PORTAGE_DEBUG" == "1" ] && set -x
diff --git a/bin/portageq b/bin/portageq
index 57a7c39..a8a3a63 100755
--- a/bin/portageq
+++ b/bin/portageq
@@ -473,6 +473,9 @@ def best_visible(argv):
 			if pkg.visible:
 				writemsg_stdout("%s\n" % (pkg.cpv,), noiselevel=-1)
 				return os.EX_OK
+
+		# No package found, write out an empty line.
+		writemsg_stdout("\n", noiselevel=-1)
 	except KeyError:
 		pass
 	return 1
@@ -480,16 +483,27 @@ best_visible.uses_root = True
 
 
 def mass_best_visible(argv):
-	"""<root> [<category/package>]+
+	"""<root> [<type>] [<category/package>]+
 	Returns category/package-version (without .ebuild).
+	The pkgtype argument defaults to "ebuild" if unspecified,
+	otherwise it must be one of ebuild, binary, or installed.
 	"""
+	type_map = {
+		"ebuild":"porttree",
+		"binary":"bintree",
+		"installed":"vartree"}
+
 	if (len(argv) < 2):
 		print("ERROR: insufficient parameters!")
 		sys.exit(2)
 	try:
-		for pack in argv[1:]:
-			mylist=portage.db[argv[0]]["porttree"].dbapi.match(pack)
-			print(pack+":"+portage.best(mylist))
+		root = argv.pop(0)
+		pkgtype = "ebuild"
+		if argv[0] in type_map:
+			pkgtype = argv.pop(0)
+		for pack in argv:
+			writemsg_stdout("%s:" % pack, noiselevel=-1)
+			best_visible([root, pkgtype, pack])
 	except KeyError:
 		sys.exit(1)
 mass_best_visible.uses_root = True
diff --git a/bin/repoman b/bin/repoman
index 10f603e..a90729c 100755
--- a/bin/repoman
+++ b/bin/repoman
@@ -1103,7 +1103,9 @@ for x in scanlist:
 			portage._doebuild_manifest_exempt_depend += 1
 			try:
 				distdir = repoman_settings['DISTDIR']
-				mf = portage.manifest.Manifest(checkdir, distdir,
+				mf = repoman_settings.repositories.get_repo_for_location(
+					os.path.dirname(os.path.dirname(checkdir)))
+				mf = mf.load_manifest(checkdir, distdir,
 					fetchlist_dict=fetchlist_dict)
 				mf.create(requiredDistfiles=None,
 					assumeDistHashesAlways=True)
@@ -1308,7 +1310,9 @@ for x in scanlist:
 				raise
 			continue
 
-	mf = Manifest(checkdir, repoman_settings["DISTDIR"])
+	mf = repoman_settings.repositories.get_repo_for_location(
+		os.path.dirname(os.path.dirname(checkdir)))
+	mf = mf.load_manifest(checkdir, repoman_settings["DISTDIR"])
 	mydigests=mf.getTypeDigests("DIST")
 
 	fetchlist_dict = portage.FetchlistDict(checkdir, repoman_settings, portdb)
diff --git a/cnf/make.globals b/cnf/make.globals
index 0be9732..df40d6e 100644
--- a/cnf/make.globals
+++ b/cnf/make.globals
@@ -101,6 +101,9 @@ PORTAGE_RSYNC_OPTS="--recursive --links --safe-links --perms --times --compress
 # message should be produced.
 PORTAGE_SYNC_STALE="30"
 
+# Executed before emerge exit if FEATURES=clean-logs is enabled.
+PORT_LOGDIR_CLEAN="find \"\${PORT_LOGDIR}\" -type f ! -name \"summary.log*\" -mtime +7 -delete"
+
 # Minimal CONFIG_PROTECT
 CONFIG_PROTECT="/etc"
 CONFIG_PROTECT_MASK="/etc/env.d"
diff --git a/man/emerge.1 b/man/emerge.1
index 835d2c0..e1df6d2 100644
--- a/man/emerge.1
+++ b/man/emerge.1
@@ -543,18 +543,16 @@ to be set in the \fBmake.conf\fR(5)
 \fBEMERGE_DEFAULT_OPTS\fR variable.
 .TP
 .BR "\-\-rebuild\-if\-new\-rev [ y | n ]"
-Rebuild packages when dependencies that are used at both build\-time and
-run\-time are built, if the dependency is not already installed with the
-same version and revision.
+Rebuild packages when build\-time dependencies are built from source, if the
+dependency is not already installed with the same version and revision.
 .TP
 .BR "\-\-rebuild\-if\-new\-ver [ y | n ]"
-Rebuild packages when dependencies that are used at both build\-time and
-run\-time are built, if the dependency is not already installed with the
-same version. Revision numbers are ignored.
+Rebuild packages when build\-time dependencies are built from source, if the
+dependency is not already installed with the same version. Revision numbers
+are ignored.
 .TP
 .BR "\-\-rebuild\-if\-unbuilt [ y | n ]"
-Rebuild packages when dependencies that are used at both build\-time and
-run\-time are built.
+Rebuild packages when build\-time dependencies are built from source.
 .TP
 .BR "\-\-rebuilt\-binaries [ y | n ]"
 Replace installed packages with binary packages that have
diff --git a/man/make.conf.5 b/man/make.conf.5
index e86dc74..a388fd4 100644
--- a/man/make.conf.5
+++ b/man/make.conf.5
@@ -198,10 +198,6 @@ non-developers as well. The \fBsandbox\fR feature is very important and
 should not be disabled by default.
 .RS
 .TP
-.B allow\-missing\-manifests
-Allow missing manifest entries. This is primarily useful for temporary
-trees or instances where manifests aren't used.
-.TP
 .B assume\-digests
 When commiting work to cvs with \fBrepoman\fR(1), assume that all existing 
 SRC_URI digests are correct.  This feature also affects digest generation via
@@ -241,6 +237,12 @@ like "File not recognized: File truncated"), try recompiling the application
 with ccache disabled before reporting a bug. Unless you are doing development
 work, do not enable ccache.
 .TP
+.B clean\-logs
+Enable automatic execution of the command specified by the
+PORT_LOGDIR_CLEAN variable. The default PORT_LOGDIR_CLEAN setting will
+remove all files from PORT_LOGDIR that were last modified at least 7
+days ago.
+.TP
 .B collision\-protect
 A QA\-feature to ensure that a package doesn't overwrite files it doesn't own.
 The \fICOLLISION_IGNORE\fR variable can be used to selectively disable this
@@ -599,6 +601,13 @@ directory does not exist, it will be created automatically and group permissions
 will be applied to it.  If the directory already exists, portage will not
 modify it's permissions.
 .TP
+.B PORT_LOGDIR_CLEAN
+This variable should contain a command for portage to call in order
+to clean PORT_LOGDIR. The command string should contain a
+\\${PORT_LOGDIR} place\-holder that will be substituted
+with the value of that variable. This variable will have no effect
+unless \fBclean\-logs\fR is enabled in \fBFEATURES\fR.
+.TP
 \fBPORTAGE_BINHOST\fR = \fI[space delimited URI list]\fR
 This is a list of hosts from which portage will grab prebuilt\-binary packages.
 Each entry in the list must specify the full address of a directory
diff --git a/man/portage.5 b/man/portage.5
index f115570..1f05d97 100644
--- a/man/portage.5
+++ b/man/portage.5
@@ -776,6 +776,13 @@ precedence over settings in \fBlayout.conf\fR, except tools such as
 masters = gentoo java-overlay
 # indicate that this repo can be used as a substitute for foo-overlay
 aliases = foo-overlay
+# do not sign manifests in this repo
+sign\-manifests = false
+# thin\-manifests only contain DIST entries
+thin\-manifests = true
+# indicate that this repo requires manifests for each package, and is
+# considered a failure if a manifest file is missing/incorrect
+use\-manifests = strict
 .fi
 .RE
 .TP
diff --git a/pym/_emerge/EbuildFetcher.py b/pym/_emerge/EbuildFetcher.py
index feb68d0..61c7848 100644
--- a/pym/_emerge/EbuildFetcher.py
+++ b/pym/_emerge/EbuildFetcher.py
@@ -21,7 +21,7 @@ class EbuildFetcher(SpawnProcess):
 
 	__slots__ = ("config_pool", "ebuild_path", "fetchonly", "fetchall",
 		"pkg", "prefetch") + \
-		("_digests", "_settings", "_uri_map")
+		("_digests", "_manifest", "_settings", "_uri_map")
 
 	def already_fetched(self, settings):
 		"""
@@ -40,7 +40,7 @@ class EbuildFetcher(SpawnProcess):
 
 		digests = self._get_digests()
 		distdir = settings["DISTDIR"]
-		allow_missing = "allow-missing-manifests" in settings.features
+		allow_missing = self._get_manifest().allow_missing
 
 		for filename in uri_map:
 			# Use stat rather than lstat since fetch() creates
@@ -179,7 +179,7 @@ class EbuildFetcher(SpawnProcess):
 			not in ('yes', 'true')
 
 		rval = 1
-		allow_missing = 'allow-missing-manifests' in self._settings.features
+		allow_missing = self._get_manifest().allow_missing
 		try:
 			if fetch(self._uri_map, self._settings, fetchonly=self.fetchonly,
 				digests=copy.deepcopy(self._get_digests()),
@@ -203,11 +203,16 @@ class EbuildFetcher(SpawnProcess):
 			raise AssertionError("ebuild not found for '%s'" % self.pkg.cpv)
 		return self.ebuild_path
 
+	def _get_manifest(self):
+		if self._manifest is None:
+			pkgdir = os.path.dirname(self._get_ebuild_path())
+			self._manifest = self.pkg.root_config.settings.repositories.get_repo_for_location(
+				os.path.dirname(os.path.dirname(pkgdir))).load_manifest(pkgdir, None)
+		return self._manifest
+
 	def _get_digests(self):
-		if self._digests is not None:
-			return self._digests
-		self._digests = portage.Manifest(os.path.dirname(
-			self._get_ebuild_path()), None).getTypeDigests("DIST")
+		if self._digests is None:
+			self._digests = self._get_manifest().getTypeDigests("DIST")
 		return self._digests
 
 	def _get_uri_map(self):
diff --git a/pym/_emerge/EbuildMetadataPhase.py b/pym/_emerge/EbuildMetadataPhase.py
index e53298b..f51b86f 100644
--- a/pym/_emerge/EbuildMetadataPhase.py
+++ b/pym/_emerge/EbuildMetadataPhase.py
@@ -20,8 +20,8 @@ class EbuildMetadataPhase(SubProcess):
 	used to extract metadata from the ebuild.
 	"""
 
-	__slots__ = ("cpv", "ebuild_path", "fd_pipes", "metadata_callback",
-		"ebuild_mtime", "metadata", "portdb", "repo_path", "settings") + \
+	__slots__ = ("cpv", "ebuild_hash", "fd_pipes", "metadata_callback",
+		"metadata", "portdb", "repo_path", "settings") + \
 		("_raw_metadata",)
 
 	_file_names = ("ebuild",)
@@ -31,7 +31,7 @@ class EbuildMetadataPhase(SubProcess):
 	def _start(self):
 		settings = self.settings
 		settings.setcpv(self.cpv)
-		ebuild_path = self.ebuild_path
+		ebuild_path = self.ebuild_hash.location
 
 		eapi = None
 		if eapi is None and \
@@ -44,8 +44,8 @@ class EbuildMetadataPhase(SubProcess):
 
 		if eapi is not None:
 			if not portage.eapi_is_supported(eapi):
-				self.metadata_callback(self.cpv, self.ebuild_path,
-					self.repo_path, {'EAPI' : eapi}, self.ebuild_mtime)
+				self.metadata_callback(self.cpv,
+					self.repo_path, {'EAPI' : eapi}, self.ebuild_hash)
 				self._set_returncode((self.pid, os.EX_OK << 8))
 				self.wait()
 				return
@@ -128,6 +128,5 @@ class EbuildMetadataPhase(SubProcess):
 			else:
 				metadata = zip(portage.auxdbkeys, metadata_lines)
 				self.metadata = self.metadata_callback(self.cpv,
-					self.ebuild_path, self.repo_path, metadata,
-					self.ebuild_mtime)
+					self.repo_path, metadata, self.ebuild_hash)
 
diff --git a/pym/_emerge/MetadataRegen.py b/pym/_emerge/MetadataRegen.py
index 8103175..b338056 100644
--- a/pym/_emerge/MetadataRegen.py
+++ b/pym/_emerge/MetadataRegen.py
@@ -3,6 +3,7 @@
 
 import portage
 from portage import os
+from portage.eclass_cache import hashed_path
 from _emerge.EbuildMetadataPhase import EbuildMetadataPhase
 from _emerge.PollScheduler import PollScheduler
 
@@ -68,16 +69,15 @@ class MetadataRegen(PollScheduler):
 				ebuild_path, repo_path = portdb.findname2(cpv)
 				if ebuild_path is None:
 					raise AssertionError("ebuild not found for '%s'" % cpv)
-				metadata, st, emtime = portdb._pull_valid_cache(
+				metadata, ebuild_hash = portdb._pull_valid_cache(
 					cpv, ebuild_path, repo_path)
 				if metadata is not None:
 					if consumer is not None:
-						consumer(cpv, ebuild_path,
-							repo_path, metadata)
+						consumer(cpv, repo_path, metadata, ebuild_hash)
 					continue
 
-				yield EbuildMetadataPhase(cpv=cpv, ebuild_path=ebuild_path,
-					ebuild_mtime=emtime,
+				yield EbuildMetadataPhase(cpv=cpv,
+					ebuild_hash=ebuild_hash,
 					metadata_callback=portdb._metadata_callback,
 					portdb=portdb, repo_path=repo_path,
 					settings=portdb.doebuild_settings)
@@ -176,9 +176,9 @@ class MetadataRegen(PollScheduler):
 			# On failure, still notify the consumer (in this case the metadata
 			# argument is None).
 			self._consumer(metadata_process.cpv,
-				metadata_process.ebuild_path,
 				metadata_process.repo_path,
-				metadata_process.metadata)
+				metadata_process.metadata,
+				metadata_process.ebuild_hash)
 
 		self._schedule()
 
diff --git a/pym/_emerge/MiscFunctionsProcess.py b/pym/_emerge/MiscFunctionsProcess.py
index ce0ab14..afa44fb 100644
--- a/pym/_emerge/MiscFunctionsProcess.py
+++ b/pym/_emerge/MiscFunctionsProcess.py
@@ -29,5 +29,11 @@ class MiscFunctionsProcess(AbstractEbuildProcess):
 		AbstractEbuildProcess._start(self)
 
 	def _spawn(self, args, **kwargs):
-		self.settings.pop("EBUILD_PHASE", None)
-		return spawn(" ".join(args), self.settings, **kwargs)
+		# Temporarily unset EBUILD_PHASE so that bashrc code doesn't
+		# think this is a real phase.
+		phase_backup = self.settings.pop("EBUILD_PHASE", None)
+		try:
+			return spawn(" ".join(args), self.settings, **kwargs)
+		finally:
+			if phase_backup is not None:
+				self.settings["EBUILD_PHASE"] = phase_backup
diff --git a/pym/_emerge/actions.py b/pym/_emerge/actions.py
index 2166963..6b65b56 100644
--- a/pym/_emerge/actions.py
+++ b/pym/_emerge/actions.py
@@ -1658,14 +1658,6 @@ def action_metadata(settings, portdb, myopts, porttrees=None):
 	porttrees_data = []
 	for path in porttrees:
 		src_db = portdb._pregen_auxdb.get(path)
-		if src_db is None and \
-			os.path.isdir(os.path.join(path, 'metadata', 'cache')):
-			src_db = portdb.metadbmodule(
-				path, 'metadata/cache', auxdbkeys, readonly=True)
-			try:
-				src_db.ec = portdb._repo_info[path].eclass_db
-			except AttributeError:
-				pass
 
 		if src_db is not None:
 			porttrees_data.append(TreeData(portdb.auxdb[path],
@@ -1704,7 +1696,6 @@ def action_metadata(settings, portdb, myopts, porttrees=None):
 	if onProgress is not None:
 		onProgress(maxval, curval)
 
-	from portage.cache.util import quiet_mirroring
 	from portage import eapi_is_supported, \
 		_validate_cache_for_unsupported_eapis
 
@@ -1713,7 +1704,6 @@ def action_metadata(settings, portdb, myopts, porttrees=None):
 	#  1) erase the progress bar
 	#  2) show the error message
 	#  3) redraw the progress bar on a new line
-	noise = quiet_mirroring()
 
 	for cp in cp_all:
 		for tree_data in porttrees_data:
@@ -1721,13 +1711,7 @@ def action_metadata(settings, portdb, myopts, porttrees=None):
 				tree_data.valid_nodes.add(cpv)
 				try:
 					src = tree_data.src_db[cpv]
-				except KeyError as e:
-					noise.missing_entry(cpv)
-					del e
-					continue
-				except CacheError as ce:
-					noise.exception(cpv, ce)
-					del ce
+				except (CacheError, KeyError):
 					continue
 
 				eapi = src.get('EAPI')
@@ -1737,8 +1721,6 @@ def action_metadata(settings, portdb, myopts, porttrees=None):
 				eapi_supported = eapi_is_supported(eapi)
 				if not eapi_supported:
 					if not _validate_cache_for_unsupported_eapis:
-						noise.misc(cpv, "unable to validate " + \
-							"cache for EAPI='%s'" % eapi)
 						continue
 
 				dest = None
@@ -1753,8 +1735,9 @@ def action_metadata(settings, portdb, myopts, porttrees=None):
 
 				if dest is not None:
 					if not (dest['_mtime_'] == src['_mtime_'] and \
-						tree_data.eclass_db.is_eclass_data_valid(
-							dest['_eclasses_']) and \
+						tree_data.eclass_db.validate_and_rewrite_cache(
+							dest['_eclasses_'], tree_data.dest_db.validation_chf,
+							tree_data.dest_db.store_eclass_paths) is not None and \
 						set(dest['_eclasses_']) == set(src['_eclasses_'])):
 						dest = None
 					else:
@@ -1775,15 +1758,13 @@ def action_metadata(settings, portdb, myopts, porttrees=None):
 				try:
 					inherited = src.get('INHERITED', '')
 					eclasses = src.get('_eclasses_')
-				except CacheError as ce:
-					noise.exception(cpv, ce)
-					del ce
+				except CacheError:
 					continue
 
 				if eclasses is not None:
-					if not tree_data.eclass_db.is_eclass_data_valid(
-						src['_eclasses_']):
-						noise.eclass_stale(cpv)
+					if tree_data.eclass_db.validate_and_rewrite_cache(
+						src['_eclasses_'], tree_data.src_db.validation_chf,
+						tree_data.src_db.store_eclass_paths) is None:
 						continue
 					inherited = eclasses
 				else:
@@ -1791,7 +1772,6 @@ def action_metadata(settings, portdb, myopts, porttrees=None):
 
 				if tree_data.src_db.complete_eclass_entries and \
 					eclasses is None:
-					noise.corruption(cpv, "missing _eclasses_ field")
 					continue
 
 				if inherited:
@@ -1801,11 +1781,9 @@ def action_metadata(settings, portdb, myopts, porttrees=None):
 						eclasses = tree_data.eclass_db.get_eclass_data(inherited)
 					except KeyError:
 						# INHERITED contains a non-existent eclass.
-						noise.eclass_stale(cpv)
 						continue
 
 					if eclasses is None:
-						noise.eclass_stale(cpv)
 						continue
 					src['_eclasses_'] = eclasses
 				else:
@@ -1820,9 +1798,9 @@ def action_metadata(settings, portdb, myopts, porttrees=None):
 
 				try:
 					tree_data.dest_db[cpv] = src
-				except CacheError as ce:
-					noise.exception(cpv, ce)
-					del ce
+				except CacheError:
+					# ignore it; can't do anything about it.
+					pass
 
 		curval += 1
 		if onProgress is not None:
diff --git a/pym/_emerge/depgraph.py b/pym/_emerge/depgraph.py
index 8b6125d..42cc659 100644
--- a/pym/_emerge/depgraph.py
+++ b/pym/_emerge/depgraph.py
@@ -174,7 +174,7 @@ class _rebuild_config(object):
 		rebuild_exclude = self._frozen_config.rebuild_exclude
 		rebuild_ignore = self._frozen_config.rebuild_ignore
 		if (self.rebuild and isinstance(parent, Package) and
-			parent.built and (priority.buildtime or priority.runtime) and
+			parent.built and priority.buildtime and
 			isinstance(dep_pkg, Package) and
 			not rebuild_exclude.findAtomForPackage(parent) and
 			not rebuild_ignore.findAtomForPackage(dep_pkg)):
@@ -209,66 +209,63 @@ class _rebuild_config(object):
 
 		return True
 
-	def _trigger_rebuild(self, parent, build_deps, runtime_deps):
+	def _trigger_rebuild(self, parent, build_deps):
 		root_slot = (parent.root, parent.slot_atom)
 		if root_slot in self.rebuild_list:
 			return False
 		trees = self._frozen_config.trees
-		children = set(build_deps).intersection(runtime_deps)
 		reinstall = False
-		for slot_atom in children:
-			kids = set([build_deps[slot_atom], runtime_deps[slot_atom]])
-			for dep_pkg in kids:
-				dep_root_slot = (dep_pkg.root, slot_atom)
-				if self._needs_rebuild(dep_pkg):
+		for slot_atom, dep_pkg in build_deps.items():
+			dep_root_slot = (dep_pkg.root, slot_atom)
+			if self._needs_rebuild(dep_pkg):
+				self.rebuild_list.add(root_slot)
+				return True
+			elif ("--usepkg" in self._frozen_config.myopts and
+				(dep_root_slot in self.reinstall_list or
+				dep_root_slot in self.rebuild_list or
+				not dep_pkg.installed)):
+
+				# A direct rebuild dependency is being installed. We
+				# should update the parent as well to the latest binary,
+				# if that binary is valid.
+				#
+				# To validate the binary, we check whether all of the
+				# rebuild dependencies are present on the same binhost.
+				#
+				# 1) If parent is present on the binhost, but one of its
+				#    rebuild dependencies is not, then the parent should
+				#    be rebuilt from source.
+				# 2) Otherwise, the parent binary is assumed to be valid,
+				#    because all of its rebuild dependencies are
+				#    consistent.
+				bintree = trees[parent.root]["bintree"]
+				uri = bintree.get_pkgindex_uri(parent.cpv)
+				dep_uri = bintree.get_pkgindex_uri(dep_pkg.cpv)
+				bindb = bintree.dbapi
+				if self.rebuild_if_new_ver and uri and uri != dep_uri:
+					cpv_norev = catpkgsplit(dep_pkg.cpv)[:-1]
+					for cpv in bindb.match(dep_pkg.slot_atom):
+						if cpv_norev == catpkgsplit(cpv)[:-1]:
+							dep_uri = bintree.get_pkgindex_uri(cpv)
+							if uri == dep_uri:
+								break
+				if uri and uri != dep_uri:
+					# 1) Remote binary package is invalid because it was
+					#    built without dep_pkg. Force rebuild.
 					self.rebuild_list.add(root_slot)
 					return True
-				elif ("--usepkg" in self._frozen_config.myopts and
-					(dep_root_slot in self.reinstall_list or
-					dep_root_slot in self.rebuild_list or
-					not dep_pkg.installed)):
-
-					# A direct rebuild dependency is being installed. We
-					# should update the parent as well to the latest binary,
-					# if that binary is valid.
-					#
-					# To validate the binary, we check whether all of the
-					# rebuild dependencies are present on the same binhost.
-					#
-					# 1) If parent is present on the binhost, but one of its
-					#    rebuild dependencies is not, then the parent should
-					#    be rebuilt from source.
-					# 2) Otherwise, the parent binary is assumed to be valid,
-					#    because all of its rebuild dependencies are
-					#    consistent.
-					bintree = trees[parent.root]["bintree"]
-					uri = bintree.get_pkgindex_uri(parent.cpv)
-					dep_uri = bintree.get_pkgindex_uri(dep_pkg.cpv)
-					bindb = bintree.dbapi
-					if self.rebuild_if_new_ver and uri and uri != dep_uri:
-						cpv_norev = catpkgsplit(dep_pkg.cpv)[:-1]
-						for cpv in bindb.match(dep_pkg.slot_atom):
-							if cpv_norev == catpkgsplit(cpv)[:-1]:
-								dep_uri = bintree.get_pkgindex_uri(cpv)
-								if uri == dep_uri:
-									break
-					if uri and uri != dep_uri:
-						# 1) Remote binary package is invalid because it was
-						#    built without dep_pkg. Force rebuild.
-						self.rebuild_list.add(root_slot)
-						return True
-					elif (parent.installed and
-						root_slot not in self.reinstall_list):
-						inst_build_time = parent.metadata.get("BUILD_TIME")
-						try:
-							bin_build_time, = bindb.aux_get(parent.cpv,
-								["BUILD_TIME"])
-						except KeyError:
-							continue
-						if bin_build_time != inst_build_time:
-							# 2) Remote binary package is valid, and local package
-							#    is not up to date. Force reinstall.
-							reinstall = True
+				elif (parent.installed and
+					root_slot not in self.reinstall_list):
+					inst_build_time = parent.metadata.get("BUILD_TIME")
+					try:
+						bin_build_time, = bindb.aux_get(parent.cpv,
+							["BUILD_TIME"])
+					except KeyError:
+						continue
+					if bin_build_time != inst_build_time:
+						# 2) Remote binary package is valid, and local package
+						#    is not up to date. Force reinstall.
+						reinstall = True
 		if reinstall:
 			self.reinstall_list.add(root_slot)
 		return reinstall
@@ -282,31 +279,15 @@ class _rebuild_config(object):
 		need_restart = False
 		graph = self._graph
 		build_deps = {}
-		runtime_deps = {}
-		leaf_nodes = deque(graph.leaf_nodes())
-
-		def ignore_non_runtime(priority):
-			return not priority.runtime
 
-		def ignore_non_buildtime(priority):
-			return not priority.buildtime
+		leaf_nodes = deque(graph.leaf_nodes())
 
 		# Trigger rebuilds bottom-up (starting with the leaves) so that parents
 		# will always know which children are being rebuilt.
 		while graph:
 			if not leaf_nodes:
-				# We're interested in intersection of buildtime and runtime,
-				# so ignore edges that do not contain both.
-				leaf_nodes.extend(graph.leaf_nodes(
-					ignore_priority=ignore_non_runtime))
-				if not leaf_nodes:
-					leaf_nodes.extend(graph.leaf_nodes(
-						ignore_priority=ignore_non_buildtime))
-					if not leaf_nodes:
-						# We'll have to drop an edge that is both
-						# buildtime and runtime. This should be
-						# quite rare.
-						leaf_nodes.append(graph.order[-1])
+				# We'll have to drop an edge. This should be quite rare.
+				leaf_nodes.append(graph.order[-1])
 
 			node = leaf_nodes.popleft()
 			if node not in graph:
@@ -315,32 +296,23 @@ class _rebuild_config(object):
 			slot_atom = node.slot_atom
 
 			# Remove our leaf node from the graph, keeping track of deps.
-			parents = graph.nodes[node][1].items()
+			parents = graph.parent_nodes(node)
 			graph.remove(node)
 			node_build_deps = build_deps.get(node, {})
-			node_runtime_deps = runtime_deps.get(node, {})
-			for parent, priorities in parents:
+			for parent in parents:
 				if parent == node:
 					# Ignore a direct cycle.
 					continue
 				parent_bdeps = build_deps.setdefault(parent, {})
-				parent_rdeps = runtime_deps.setdefault(parent, {})
-				for priority in priorities:
-					if priority.buildtime:
-						parent_bdeps[slot_atom] = node
-					if priority.runtime:
-						parent_rdeps[slot_atom] = node
-				if slot_atom in parent_bdeps and slot_atom in parent_rdeps:
-					parent_rdeps.update(node_runtime_deps)
+				parent_bdeps[slot_atom] = node
 				if not graph.child_nodes(parent):
 					leaf_nodes.append(parent)
 
 			# Trigger rebuilds for our leaf node. Because all of our children
-			# have been processed, build_deps and runtime_deps will be
-			# completely filled in, and self.rebuild_list / self.reinstall_list
-			# will tell us whether any of our children need to be rebuilt or
-			# reinstalled.
-			if self._trigger_rebuild(node, node_build_deps, node_runtime_deps):
+			# have been processed, the build_deps will be completely filled in,
+			# and self.rebuild_list / self.reinstall_list will tell us whether
+			# any of our children need to be rebuilt or reinstalled.
+			if self._trigger_rebuild(node, node_build_deps):
 				need_restart = True
 
 		return need_restart
diff --git a/pym/_emerge/help.py b/pym/_emerge/help.py
index c978ce2..57b376d 100644
--- a/pym/_emerge/help.py
+++ b/pym/_emerge/help.py
@@ -641,26 +641,24 @@ def help(myopts, havecolor=1):
 		print()
 		print("       " + green("--rebuild-if-new-rev") + " [ %s | %s ]" % \
 			(turquoise("y"), turquoise("n")))
-		desc = "Rebuild packages when dependencies that are " + \
-			"used at both build-time and run-time are built, " + \
-			"if the dependency is not already installed with the " + \
-			"same version and revision."
+		desc = "Rebuild packages when build-time dependencies are built " + \
+			"from source, if the dependency is not already installed with " + \
+			"the same version and revision."
 		for line in wrap(desc, desc_width):
 			print(desc_indent + line)
 		print()
 		print("       " + green("--rebuild-if-new-ver") + " [ %s | %s ]" % \
 			(turquoise("y"), turquoise("n")))
-		desc = "Rebuild packages when dependencies that are " + \
-			"used at both build-time and run-time are built, " + \
-			"if the dependency is not already installed with the " + \
-			"same version. Revision numbers are ignored."
+		desc = "Rebuild packages when build-time dependencies are built " + \
+			"from source, if the dependency is not already installed with " + \
+			"the same version. Revision numbers are ignored."
 		for line in wrap(desc, desc_width):
 			print(desc_indent + line)
 		print()
 		print("       " + green("--rebuild-if-unbuilt") + " [ %s | %s ]" % \
 			(turquoise("y"), turquoise("n")))
-		desc = "Rebuild packages when dependencies that are " + \
-			"used at both build-time and run-time are built."
+		desc = "Rebuild packages when build-time dependencies are built " + \
+			"from source"
 		for line in wrap(desc, desc_width):
 			print(desc_indent + line)
 		print()
diff --git a/pym/_emerge/main.py b/pym/_emerge/main.py
index 2830214..4cabafd 100644
--- a/pym/_emerge/main.py
+++ b/pym/_emerge/main.py
@@ -28,7 +28,8 @@ import portage.exception
 from portage.data import secpass
 from portage.dbapi.dep_expand import dep_expand
 from portage.util import normalize_path as normpath
-from portage.util import shlex_split, writemsg_level, writemsg_stdout
+from portage.util import (shlex_split, varexpand,
+	writemsg_level, writemsg_stdout)
 from portage._sets import SETPREFIX
 from portage._global_updates import _global_updates
 
@@ -388,6 +389,8 @@ def post_emerge(myaction, myopts, myfiles,
 				" %s spawn failed of %s\n" % (bad("*"), postemerge,),
 				level=logging.ERROR, noiselevel=-1)
 
+	clean_logs(settings)
+
 	if "--quiet" not in myopts and \
 		myaction is None and "@world" in myfiles:
 		show_depclean_suggestion()
@@ -1222,7 +1225,6 @@ def ionice(settings):
 	if not ionice_cmd:
 		return
 
-	from portage.util import varexpand
 	variables = {"PID" : str(os.getpid())}
 	cmd = [varexpand(x, mydict=variables) for x in ionice_cmd]
 
@@ -1238,6 +1240,35 @@ def ionice(settings):
 		out.eerror("PORTAGE_IONICE_COMMAND returned %d" % (rval,))
 		out.eerror("See the make.conf(5) man page for PORTAGE_IONICE_COMMAND usage instructions.")
 
+def clean_logs(settings):
+
+	if "clean-logs" not in settings.features:
+		return
+
+	clean_cmd = settings.get("PORT_LOGDIR_CLEAN")
+	if clean_cmd:
+		clean_cmd = shlex_split(clean_cmd)
+	if not clean_cmd:
+		return
+
+	logdir = settings.get("PORT_LOGDIR")
+	if logdir is None or not os.path.isdir(logdir):
+		return
+
+	variables = {"PORT_LOGDIR" : logdir}
+	cmd = [varexpand(x, mydict=variables) for x in clean_cmd]
+
+	try:
+		rval = portage.process.spawn(cmd, env=os.environ)
+	except portage.exception.CommandNotFound:
+		rval = 127
+
+	if rval != os.EX_OK:
+		out = portage.output.EOutput()
+		out.eerror("PORT_LOGDIR_CLEAN returned %d" % (rval,))
+		out.eerror("See the make.conf(5) man page for "
+			"PORT_LOGDIR_CLEAN usage instructions.")
+
 def setconfig_fallback(root_config):
 	from portage._sets.base import DummyPackageSet
 	from portage._sets.files import WorldSelectedSet
diff --git a/pym/_emerge/search.py b/pym/_emerge/search.py
index 35f0412..3fed2b6 100644
--- a/pym/_emerge/search.py
+++ b/pym/_emerge/search.py
@@ -305,8 +305,9 @@ class search(object):
 					myebuild = self._findname(mycpv)
 					if myebuild:
 						pkgdir = os.path.dirname(myebuild)
-						from portage import manifest
-						mf = manifest.Manifest(
+						mf = self.settings.repositories.get_repo_for_location(
+							os.path.dirname(os.path.dirname(pkgdir)))
+						mf = mf.load_manifest(
 							pkgdir, self.settings["DISTDIR"])
 						try:
 							uri_map = self._getFetchMap(mycpv)
diff --git a/pym/portage/cache/flat_hash.py b/pym/portage/cache/flat_hash.py
index b6bc074..2eae9f6 100644
--- a/pym/portage/cache/flat_hash.py
+++ b/pym/portage/cache/flat_hash.py
@@ -31,7 +31,7 @@ class database(fs_template.FsBased):
 			self.label.lstrip(os.path.sep).rstrip(os.path.sep))
 		write_keys = set(self._known_keys)
 		write_keys.add("_eclasses_")
-		write_keys.add("_mtime_")
+		write_keys.add("_%s_" % (self.validation_chf,))
 		self._write_keys = sorted(write_keys)
 		if not self.readonly and not os.path.exists(self.location):
 			self._ensure_dirs()
@@ -69,7 +69,6 @@ class database(fs_template.FsBased):
 			raise cache_errors.CacheCorruption(cpv, e)
 
 	def _setitem(self, cpv, values):
-#		import pdb;pdb.set_trace()
 		s = cpv.rfind("/")
 		fp = os.path.join(self.location,cpv[:s],".update.%i.%s" % (os.getpid(), cpv[s+1:]))
 		try:
@@ -153,3 +152,9 @@ class database(fs_template.FsBased):
 						dirs.append((depth+1, p))
 					continue
 				yield p[len_base+1:]
+
+
+class md5_database(database):
+
+	validation_chf = 'md5'
+	store_eclass_paths = False
diff --git a/pym/portage/cache/metadata.py b/pym/portage/cache/metadata.py
index 4c735d7..07ec20e 100644
--- a/pym/portage/cache/metadata.py
+++ b/pym/portage/cache/metadata.py
@@ -6,6 +6,7 @@ import errno
 import re
 import stat
 import sys
+from operator import attrgetter
 from portage import os
 from portage import _encodings
 from portage import _unicode_encode
@@ -63,9 +64,11 @@ class database(flat_hash.database):
 			if "INHERITED" in d:
 				if self.ec is None:
 					self.ec = portage.eclass_cache.cache(self.location[:-15])
+				getter = attrgetter(self.validation_chf)
 				try:
-					d["_eclasses_"] = self.ec.get_eclass_data(
-						d["INHERITED"].split())
+					ec_data = self.ec.get_eclass_data(d["INHERITED"].split())
+					d["_eclasses_"] = dict((k, (v.eclass_dir, getter(v)))
+						for k,v in ec_data.items())
 				except KeyError as e:
 					# INHERITED contains a non-existent eclass.
 					raise cache_errors.CacheCorruption(cpv, e)
diff --git a/pym/portage/cache/metadata_overlay.py b/pym/portage/cache/metadata_overlay.py
deleted file mode 100644
index cfa0051..0000000
--- a/pym/portage/cache/metadata_overlay.py
+++ /dev/null
@@ -1,105 +0,0 @@
-# Copyright 1999-2010 Gentoo Foundation
-# Distributed under the terms of the GNU General Public License v2
-
-from portage.cache import template
-from portage.cache.cache_errors import CacheCorruption
-from portage.cache.flat_hash import database as db_rw
-from portage.cache.metadata import database as db_ro
-
-class database(template.database):
-
-	serialize_eclasses = False
-
-	def __init__(self, location, label, auxdbkeys, db_rw=db_rw, db_ro=db_ro,
-		*args, **config):
-		super_config = config.copy()
-		super_config.pop("gid", None)
-		super_config.pop("perms", None)
-		super(database, self).__init__(location, label, auxdbkeys,
-			*args, **super_config)
-		self.db_rw = db_rw(location, label, auxdbkeys, **config)
-		self.commit = self.db_rw.commit
-		self.autocommits = self.db_rw.autocommits
-		if isinstance(db_ro, type):
-			ro_config = config.copy()
-			ro_config["readonly"] = True
-			self.db_ro = db_ro(label, "metadata/cache", auxdbkeys, **ro_config)
-		else:
-			self.db_ro = db_ro
-
-	def __getitem__(self, cpv):
-		"""funnel whiteout validation through here, since value needs to be fetched"""
-		try:
-			value = self.db_rw[cpv]
-		except KeyError:
-			return self.db_ro[cpv] # raises a KeyError when necessary
-		except CacheCorruption:
-			del self.db_rw[cpv]
-			return self.db_ro[cpv] # raises a KeyError when necessary
-		if self._is_whiteout(value):
-			if self._is_whiteout_valid(cpv, value):
-				raise KeyError(cpv)
-			else:
-				del self.db_rw[cpv]
-				return self.db_ro[cpv] # raises a KeyError when necessary
-		else:
-			return value
-
-	def _setitem(self, name, values):
-		try:
-			value_ro = self.db_ro.get(name)
-		except CacheCorruption:
-			value_ro = None
-		if value_ro is not None and \
-			self._are_values_identical(value_ro, values):
-			# we have matching values in the underlying db_ro
-			# so it is unnecessary to store data in db_rw
-			try:
-				del self.db_rw[name] # delete unwanted whiteout when necessary
-			except KeyError:
-				pass
-			return
-		self.db_rw[name] = values
-
-	def _delitem(self, cpv):
-		value = self[cpv] # validates whiteout and/or raises a KeyError when necessary
-		if cpv in self.db_ro:
-			self.db_rw[cpv] = self._create_whiteout(value)
-		else:
-			del self.db_rw[cpv]
-
-	def __contains__(self, cpv):
-		try:
-			self[cpv] # validates whiteout when necessary
-		except KeyError:
-			return False
-		return True
-
-	def __iter__(self):
-		s = set()
-		for cpv in self.db_rw:
-			if cpv in self: # validates whiteout when necessary
-				yield cpv
-			# set includes whiteouts so they won't be yielded later
-			s.add(cpv)
-		for cpv in self.db_ro:
-			if cpv not in s:
-				yield cpv
-
-	def _is_whiteout(self, value):
-		return value["EAPI"] == "whiteout"
-
-	def _create_whiteout(self, value):
-		return {"EAPI":"whiteout","_eclasses_":value["_eclasses_"],"_mtime_":value["_mtime_"]}
-
-	def _is_whiteout_valid(self, name, value_rw):
-		try:
-			value_ro = self.db_ro[name]
-			return self._are_values_identical(value_rw,value_ro)
-		except KeyError:
-			return False
-
-	def _are_values_identical(self, value1, value2):
-		if value1['_mtime_'] != value2['_mtime_']:
-			return False
-		return value1["_eclasses_"] == value2["_eclasses_"]
diff --git a/pym/portage/cache/template.py b/pym/portage/cache/template.py
index f84d8f4..515ba02 100644
--- a/pym/portage/cache/template.py
+++ b/pym/portage/cache/template.py
@@ -7,6 +7,7 @@ from portage.cache.cache_errors import InvalidRestriction
 from portage.cache.mappings import ProtectedDict
 import sys
 import warnings
+import operator
 
 if sys.hexversion >= 0x3000000:
 	basestring = str
@@ -21,6 +22,8 @@ class database(object):
 	autocommits = False
 	cleanse_keys = False
 	serialize_eclasses = True
+	validation_chf = 'mtime'
+	store_eclass_paths = True
 
 	def __init__(self, location, label, auxdbkeys, readonly=False):
 		""" initialize the derived class; specifically, store label/keys"""
@@ -40,7 +43,8 @@ class database(object):
 			self.updates = 0
 		d=self._getitem(cpv)
 		if self.serialize_eclasses and "_eclasses_" in d:
-			d["_eclasses_"] = reconstruct_eclasses(cpv, d["_eclasses_"])
+			d["_eclasses_"] = reconstruct_eclasses(cpv, d["_eclasses_"],
+				self.validation_chf, paths=self.store_eclass_paths)
 		elif "_eclasses_" not in d:
 			d["_eclasses_"] = {}
 		mtime = d.get('_mtime_')
@@ -60,22 +64,46 @@ class database(object):
 		override this in derived classess"""
 		raise NotImplementedError
 
+	@staticmethod
+	def _internal_eclasses(extern_ec_dict, chf_type, paths):
+		"""
+		When serialize_eclasses is False, we have to convert an external
+		eclass dict containing hashed_path objects into an appropriate
+		internal dict containing values of chf_type (and eclass dirs
+		if store_eclass_paths is True).
+		"""
+		if not extern_ec_dict:
+			return extern_ec_dict
+		chf_getter = operator.attrgetter(chf_type)
+		if paths:
+			intern_ec_dict = dict((k, (v.eclass_dir, chf_getter(v)))
+				for k, v in extern_ec_dict.items())
+		else:
+			intern_ec_dict = dict((k, chf_getter(v))
+				for k, v in extern_ec_dict.items())
+		return intern_ec_dict
+
 	def __setitem__(self, cpv, values):
 		"""set a cpv to values
 		This shouldn't be overriden in derived classes since it handles the readonly checks"""
 		if self.readonly:
 			raise cache_errors.ReadOnlyRestriction()
+		d = None
 		if self.cleanse_keys:
 			d=ProtectedDict(values)
 			for k, v in list(d.items()):
 				if not v:
 					del d[k]
-			if self.serialize_eclasses and "_eclasses_" in values:
-				d["_eclasses_"] = serialize_eclasses(d["_eclasses_"])
-		elif self.serialize_eclasses and "_eclasses_" in values:
-			d = ProtectedDict(values)
-			d["_eclasses_"] = serialize_eclasses(d["_eclasses_"])
-		else:
+		if "_eclasses_" in values:
+			if d is None:
+				d = ProtectedDict(values)
+			if self.serialize_eclasses:
+				d["_eclasses_"] = serialize_eclasses(d["_eclasses_"],
+					self.validation_chf, paths=self.store_eclass_paths)
+			else:
+				d["_eclasses_"] = self._internal_eclasses(d["_eclasses_"],
+					self.validation_chf, self.store_eclass_paths)
+		elif d is None:
 			d = values
 		self._setitem(cpv, d)
 		if not self.autocommits:
@@ -159,6 +187,18 @@ class database(object):
 		except KeyError:
 			return x
 
+	def validate_entry(self, entry, ebuild_hash, eclass_db):
+		hash_key = '_%s_' % self.validation_chf
+		if entry[hash_key] != getattr(ebuild_hash, self.validation_chf):
+			return False
+		update = eclass_db.validate_and_rewrite_cache(entry['_eclasses_'], self.validation_chf,
+			self.store_eclass_paths)
+		if update is None:
+			return False
+		if update:
+			entry['_eclasses_'] = update
+		return True
+
 	def get_matches(self, match_dict):
 		"""generic function for walking the entire cache db, matching restrictions to
 		filter what cpv's are returned.  Derived classes should override this if they
@@ -195,7 +235,9 @@ class database(object):
 		keys = __iter__
 		items = iteritems
 
-def serialize_eclasses(eclass_dict):
+_keysorter = operator.itemgetter(0)
+
+def serialize_eclasses(eclass_dict, chf_type='mtime', paths=True):
 	"""takes a dict, returns a string representing said dict"""
 	"""The "new format", which causes older versions of <portage-2.1.2 to
 	traceback with a ValueError due to failed long() conversion.  This format
@@ -206,27 +248,40 @@ def serialize_eclasses(eclass_dict):
 	"""
 	if not eclass_dict:
 		return ""
-	return "\t".join(k + "\t%s\t%s" % eclass_dict[k] \
-		for k in sorted(eclass_dict))
+	getter = operator.attrgetter(chf_type)
+	if paths:
+		return "\t".join("%s\t%s\t%s" % (k, v.eclass_dir, getter(v))
+			for k, v in sorted(eclass_dict.items(), key=_keysorter))
+	return "\t".join("%s\t%s" % (k, getter(v))
+		for k, v in sorted(eclass_dict.items(), key=_keysorter))
+
 
-def reconstruct_eclasses(cpv, eclass_string):
+def reconstruct_eclasses(cpv, eclass_string, chf_type='mtime', paths=True):
 	"""returns a dict when handed a string generated by serialize_eclasses"""
 	eclasses = eclass_string.rstrip().lstrip().split("\t")
 	if eclasses == [""]:
 		# occasionally this occurs in the fs backends.  they suck.
 		return {}
-	
-	if len(eclasses) % 2 != 0 and len(eclasses) % 3 != 0:
+
+	converter = str
+	if chf_type == 'mtime':
+		converter = long
+
+	if paths:
+		if len(eclasses) % 3 != 0:
+			raise cache_errors.CacheCorruption(cpv, "_eclasses_ was of invalid len %i" % len(eclasses))
+	elif len(eclasses) % 2 != 0:
 		raise cache_errors.CacheCorruption(cpv, "_eclasses_ was of invalid len %i" % len(eclasses))
 	d={}
 	try:
-		if eclasses[1].isdigit():
-			for x in range(0, len(eclasses), 2):
-				d[eclasses[x]] = ("", long(eclasses[x + 1]))
-		else:
+		i = iter(eclasses)
+		if paths:
 			# The old format contains paths that will be discarded.
-			for x in range(0, len(eclasses), 3):
-				d[eclasses[x]] = (eclasses[x + 1], long(eclasses[x + 2]))
+			for name, path, val in zip(i, i, i):
+				d[name] = (path, converter(val))
+		else:
+			for name, val in zip(i, i):
+				d[name] = converter(val)
 	except IndexError:
 		raise cache_errors.CacheCorruption(cpv,
 			"_eclasses_ was of invalid len %i" % len(eclasses))
diff --git a/pym/portage/cache/util.py b/pym/portage/cache/util.py
deleted file mode 100644
index b824689..0000000
--- a/pym/portage/cache/util.py
+++ /dev/null
@@ -1,170 +0,0 @@
-# Copyright: 2005 Gentoo Foundation
-# Author(s): Brian Harring (ferringb@gentoo.org)
-# License: GPL2
-
-from __future__ import print_function
-
-__all__ = ["mirror_cache", "non_quiet_mirroring", "quiet_mirroring"]
-
-from itertools import chain
-from portage.cache import cache_errors
-from portage.localization import _
-
-def mirror_cache(valid_nodes_iterable, src_cache, trg_cache, eclass_cache=None, verbose_instance=None):
-
-	from portage import eapi_is_supported, \
-		_validate_cache_for_unsupported_eapis
-	if not src_cache.complete_eclass_entries and not eclass_cache:
-		raise Exception("eclass_cache required for cache's of class %s!" % src_cache.__class__)
-
-	if verbose_instance == None:
-		noise=quiet_mirroring()
-	else:
-		noise=verbose_instance
-
-	dead_nodes = set(trg_cache)
-	count=0
-
-	if not trg_cache.autocommits:
-		trg_cache.sync(100)
-
-	for x in valid_nodes_iterable:
-#		print "processing x=",x
-		count+=1
-		dead_nodes.discard(x)
-		try:
-			entry = src_cache[x]
-		except KeyError as e:
-			noise.missing_entry(x)
-			del e
-			continue
-		except cache_errors.CacheError as ce:
-			noise.exception(x, ce)
-			del ce
-			continue
-
-		eapi = entry.get('EAPI')
-		if not eapi:
-			eapi = '0'
-		eapi = eapi.lstrip('-')
-		eapi_supported = eapi_is_supported(eapi)
-		if not eapi_supported:
-			if not _validate_cache_for_unsupported_eapis:
-				noise.misc(x, _("unable to validate cache for EAPI='%s'") % eapi)
-				continue
-
-		write_it = True
-		trg = None
-		try:
-			trg = trg_cache[x]
-		except (KeyError, cache_errors.CacheError):
-			pass
-		else:
-			if trg['_mtime_'] == entry['_mtime_'] and \
-				eclass_cache.is_eclass_data_valid(trg['_eclasses_']) and \
-				set(trg['_eclasses_']) == set(entry['_eclasses_']):
-				write_it = False
-
-		for d in (entry, trg):
-			if d is not None and d.get('EAPI') in ('', '0'):
-				del d['EAPI']
-
-		if trg and not write_it:
-			""" We don't want to skip the write unless we're really sure that
-			the existing cache is identical, so don't trust _mtime_ and
-			_eclasses_ alone."""
-			for k in set(chain(entry, trg)).difference(
-				("_mtime_", "_eclasses_")):
-				if trg.get(k, "") != entry.get(k, ""):
-					write_it = True
-					break
-
-		if write_it:
-			try:
-				inherited = entry.get("INHERITED", "")
-				eclasses = entry.get("_eclasses_")
-			except cache_errors.CacheError as ce:
-				noise.exception(x, ce)
-				del ce
-				continue
-
-			if eclasses is not None:
-				if not eclass_cache.is_eclass_data_valid(entry["_eclasses_"]):
-					noise.eclass_stale(x)
-					continue
-				inherited = eclasses
-			else:
-				inherited = inherited.split()
-
-			if inherited:
-				if src_cache.complete_eclass_entries and eclasses is None:
-					noise.corruption(x, "missing _eclasses_ field")
-					continue
-
-				# Even if _eclasses_ already exists, replace it with data from
-				# eclass_cache, in order to insert local eclass paths.
-				try:
-					eclasses = eclass_cache.get_eclass_data(inherited)
-				except KeyError:
-					# INHERITED contains a non-existent eclass.
-					noise.eclass_stale(x)
-					continue
-
-				if eclasses is None:
-					noise.eclass_stale(x)
-					continue
-				entry["_eclasses_"] = eclasses
-
-			if not eapi_supported:
-				for k in set(entry).difference(("_mtime_", "_eclasses_")):
-					entry[k] = ""
-				entry["EAPI"] = "-" + eapi
-
-			# by this time, if it reaches here, the eclass has been validated, and the entry has 
-			# been updated/translated (if needs be, for metadata/cache mainly)
-			try:
-				trg_cache[x] = entry
-			except cache_errors.CacheError as ce:
-				noise.exception(x, ce)
-				del ce
-				continue
-		if count >= noise.call_update_min:
-			noise.update(x)
-			count = 0
-
-	if not trg_cache.autocommits:
-		trg_cache.commit()
-
-	# ok.  by this time, the trg_cache is up to date, and we have a dict
-	# with a crapload of cpv's.  we now walk the target db, removing stuff if it's in the list.
-	for key in dead_nodes:
-		try:
-			del trg_cache[key]
-		except KeyError:
-			pass
-		except cache_errors.CacheError as ce:
-			noise.exception(ce)
-			del ce
-	noise.finish()
-
-
-class quiet_mirroring(object):
-	# call_update_every is used by mirror_cache to determine how often to call in.
-	# quiet defaults to 2^24 -1.  Don't call update, 'cept once every 16 million or so :)
-	call_update_min = 0xffffff
-	def update(self,key,*arg):		pass
-	def exception(self,key,*arg):	pass
-	def eclass_stale(self,*arg):	pass
-	def missing_entry(self, key):	pass
-	def misc(self,key,*arg):		pass
-	def corruption(self, key, s):	pass
-	def finish(self, *arg):			pass
-	
-class non_quiet_mirroring(quiet_mirroring):
-	call_update_min=1
-	def update(self,key,*arg):	print("processed",key)
-	def exception(self, key, *arg):	print("exec",key,arg)
-	def missing(self,key):		print("key %s is missing", key)
-	def corruption(self,key,*arg):	print("corrupt %s:" % key,arg)
-	def eclass_stale(self,key,*arg):print("stale %s:"%key,arg)
-
diff --git a/pym/portage/cache/volatile.py b/pym/portage/cache/volatile.py
index 0bf6bab..f96788d 100644
--- a/pym/portage/cache/volatile.py
+++ b/pym/portage/cache/volatile.py
@@ -8,6 +8,7 @@ class database(template.database):
 
 	autocommits = True
 	serialize_eclasses = False
+	store_eclass_paths = False
 
 	def __init__(self, *args, **config):
 		config.pop("gid", None)
@@ -21,5 +22,5 @@ class database(template.database):
 	def _setitem(self, name, values):
 		self._data[name] = copy.deepcopy(values)
 
-	def _getitem(self, cpv):
+	def __getitem__(self, cpv):
 		return copy.deepcopy(self._data[cpv])
diff --git a/pym/portage/checksum.py b/pym/portage/checksum.py
index 9e7e455..ef90bf6 100644
--- a/pym/portage/checksum.py
+++ b/pym/portage/checksum.py
@@ -255,8 +255,10 @@ def perform_checksum(filename, hashname="MD5", calc_prelink=0):
 					" hash function not available (needs dev-python/pycrypto)")
 			myhash, mysize = hashfunc_map[hashname](myfilename)
 		except (OSError, IOError) as e:
-			if e.errno == errno.ENOENT:
+			if e.errno in (errno.ENOENT, errno.ESTALE):
 				raise portage.exception.FileNotFound(myfilename)
+			elif e.errno == portage.exception.PermissionDenied.errno:
+				raise portage.exception.PermissionDenied(myfilename)
 			raise
 		return myhash, mysize
 	finally:
diff --git a/pym/portage/const.py b/pym/portage/const.py
index f108176..bfcdbef 100644
--- a/pym/portage/const.py
+++ b/pym/portage/const.py
@@ -86,16 +86,16 @@ EBUILD_PHASES            = ("pretend", "setup", "unpack", "prepare", "configure"
                            "package", "preinst", "postinst","prerm", "postrm",
                            "nofetch", "config", "info", "other")
 SUPPORTED_FEATURES       = frozenset([
-                           "allow-missing-manifests",
                            "assume-digests", "binpkg-logs", "buildpkg", "buildsyspkg", "candy",
-                           "ccache", "chflags", "collision-protect", "compress-build-logs",
+                           "ccache", "chflags", "clean-logs",
+                           "collision-protect", "compress-build-logs",
                            "digest", "distcc", "distcc-pump", "distlocks", "ebuild-locks", "fakeroot",
                            "fail-clean", "fixpackages", "force-mirror", "getbinpkg",
                            "installsources", "keeptemp", "keepwork", "fixlafiles", "lmirror",
                            "metadata-transfer", "mirror", "multilib-strict", "news",
                            "noauto", "noclean", "nodoc", "noinfo", "noman",
-                           "nostrip", "notitles", "parallel-fetch", "parallel-install",
-                           "parse-eapi-ebuild-head",
+                           "nostrip", "notitles", "no-env-update", "parallel-fetch",
+                           "parallel-install", "parse-eapi-ebuild-head",
                            "prelink-checksums", "preserve-libs",
                            "protect-owned", "python-trace", "sandbox",
                            "selinux", "sesandbox", "sfperms",
diff --git a/pym/portage/dbapi/porttree.py b/pym/portage/dbapi/porttree.py
index bf8ecd9..46524e4 100644
--- a/pym/portage/dbapi/porttree.py
+++ b/pym/portage/dbapi/porttree.py
@@ -17,7 +17,7 @@ portage.proxy.lazyimport.lazyimport(globals(),
 	'portage.versions:best,catpkgsplit,_pkgsplit@pkgsplit,ver_regexp',
 )
 
-from portage.cache import metadata_overlay, volatile
+from portage.cache import volatile
 from portage.cache.cache_errors import CacheError
 from portage.cache.mappings import Mapping
 from portage.dbapi import dbapi
@@ -121,8 +121,6 @@ class portdbapi(dbapi):
 		self._have_root_eclass_dir = os.path.isdir(
 			os.path.join(self.settings.repositories.mainRepoLocation(), "eclass"))
 
-		self.metadbmodule = self.settings.load_best_module("portdbapi.metadbmodule")
-
 		#if the portdbapi is "frozen", then we assume that we can cache everything (that no updates to it are happening)
 		self.xcache = {}
 		self.frozen = 0
@@ -153,6 +151,9 @@ class portdbapi(dbapi):
 		self.auxdbmodule = self.settings.load_best_module("portdbapi.auxdbmodule")
 		self.auxdb = {}
 		self._pregen_auxdb = {}
+		# If the current user doesn't have depcachedir write permission,
+		# then the depcachedir cache is kept here read-only access.
+		self._ro_auxdb = {}
 		self._init_cache_dirs()
 		depcachedir_w_ok = os.access(self.depcachedir, os.W_OK)
 		cache_kwargs = {
@@ -169,18 +170,14 @@ class portdbapi(dbapi):
 		# to the cache entries/directories.
 		if secpass < 1 or not depcachedir_w_ok:
 			for x in self.porttrees:
+				self.auxdb[x] = volatile.database(
+					self.depcachedir, x, filtered_auxdbkeys,
+					**cache_kwargs)
 				try:
-					db_ro = self.auxdbmodule(self.depcachedir, x,
+					self._ro_auxdb[x] = self.auxdbmodule(self.depcachedir, x,
 						filtered_auxdbkeys, readonly=True, **cache_kwargs)
 				except CacheError:
-					self.auxdb[x] = volatile.database(
-						self.depcachedir, x, filtered_auxdbkeys,
-						**cache_kwargs)
-				else:
-					self.auxdb[x] = metadata_overlay.database(
-						self.depcachedir, x, filtered_auxdbkeys,
-						db_rw=volatile.database, db_ro=db_ro,
-						**cache_kwargs)
+					pass
 		else:
 			for x in self.porttrees:
 				if x in self.auxdb:
@@ -188,17 +185,16 @@ class portdbapi(dbapi):
 				# location, label, auxdbkeys
 				self.auxdb[x] = self.auxdbmodule(
 					self.depcachedir, x, filtered_auxdbkeys, **cache_kwargs)
-				if self.auxdbmodule is metadata_overlay.database:
-					self.auxdb[x].db_ro.ec = self._repo_info[x].eclass_db
 		if "metadata-transfer" not in self.settings.features:
 			for x in self.porttrees:
 				if x in self._pregen_auxdb:
 					continue
-				if os.path.isdir(os.path.join(x, "metadata", "cache")):
-					self._pregen_auxdb[x] = self.metadbmodule(
-						x, "metadata/cache", filtered_auxdbkeys, readonly=True)
+				conf = self.repositories.get_repo_for_location(x)
+				cache = conf.get_pregenerated_cache(filtered_auxdbkeys, readonly=True)
+				if cache is not None:
+					self._pregen_auxdb[x] = cache
 					try:
-						self._pregen_auxdb[x].ec = self._repo_info[x].eclass_db
+						cache.ec = self._repo_info[x].eclass_db
 					except AttributeError:
 						pass
 		# Selectively cache metadata in order to optimize dep matching.
@@ -340,16 +336,16 @@ class portdbapi(dbapi):
 		@returns: A new EbuildMetadataPhase instance, or None if the
 			metadata cache is already valid.
 		"""
-		metadata, st, emtime = self._pull_valid_cache(cpv, ebuild_path, repo_path)
+		metadata, ebuild_hash = self._pull_valid_cache(cpv, ebuild_path, repo_path)
 		if metadata is not None:
 			return None
 
-		process = EbuildMetadataPhase(cpv=cpv, ebuild_path=ebuild_path,
-			ebuild_mtime=emtime, metadata_callback=self._metadata_callback,
+		process = EbuildMetadataPhase(cpv=cpv,
+			ebuild_hash=ebuild_hash, metadata_callback=self._metadata_callback,
 			portdb=self, repo_path=repo_path, settings=self.doebuild_settings)
 		return process
 
-	def _metadata_callback(self, cpv, ebuild_path, repo_path, metadata, mtime):
+	def _metadata_callback(self, cpv, repo_path, metadata, ebuild_hash):
 
 		i = metadata
 		if hasattr(metadata, "items"):
@@ -362,8 +358,17 @@ class portdbapi(dbapi):
 		else:
 			metadata["_eclasses_"] = {}
 
+		try:
+			cache = self.auxdb[repo_path]
+			chf = cache.validation_chf
+			metadata['_%s_' % chf] = getattr(ebuild_hash, chf)
+		except CacheError:
+			# Normally this shouldn't happen, so we'll show
+			# a traceback for debugging purposes.
+			traceback.print_exc()
+			cache = None
+
 		metadata.pop("INHERITED", None)
-		metadata["_mtime_"] = mtime
 
 		eapi = metadata.get("EAPI")
 		if not eapi or not eapi.strip():
@@ -374,21 +379,22 @@ class portdbapi(dbapi):
 				metadata[k] = ""
 			metadata["EAPI"] = "-" + eapi.lstrip("-")
 
-		try:
-			self.auxdb[repo_path][cpv] = metadata
-		except CacheError:
-			# Normally this shouldn't happen, so we'll show
-			# a traceback for debugging purposes.
-			traceback.print_exc()
+		if cache is not None:
+			try:
+				cache[cpv] = metadata
+			except CacheError:
+				# Normally this shouldn't happen, so we'll show
+				# a traceback for debugging purposes.
+				traceback.print_exc()
 		return metadata
 
 	def _pull_valid_cache(self, cpv, ebuild_path, repo_path):
 		try:
-			# Don't use unicode-wrapped os module, for better performance.
-			st = _os.stat(_unicode_encode(ebuild_path,
-				encoding=_encodings['fs'], errors='strict'))
-			emtime = st[stat.ST_MTIME]
-		except OSError:
+			ebuild_hash = eclass_cache.hashed_path(ebuild_path)
+			# snag mtime since we use it later, and to trigger stat failure
+			# if it doesn't exist
+			ebuild_hash.mtime
+		except FileNotFound:
 			writemsg(_("!!! aux_get(): ebuild for " \
 				"'%s' does not exist at:\n") % (cpv,), noiselevel=-1)
 			writemsg("!!!            %s\n" % ebuild_path, noiselevel=-1)
@@ -401,39 +407,35 @@ class portdbapi(dbapi):
 		pregen_auxdb = self._pregen_auxdb.get(repo_path)
 		if pregen_auxdb is not None:
 			auxdbs.append(pregen_auxdb)
+		ro_auxdb = self._ro_auxdb.get(repo_path)
+		if ro_auxdb is not None:
+			auxdbs.append(ro_auxdb)
 		auxdbs.append(self.auxdb[repo_path])
 		eclass_db = self._repo_info[repo_path].eclass_db
 
-		doregen = True
 		for auxdb in auxdbs:
 			try:
 				metadata = auxdb[cpv]
 			except KeyError:
-				pass
+				continue
 			except CacheError:
-				if auxdb is not pregen_auxdb:
+				if not auxdb.readonly:
 					try:
 						del auxdb[cpv]
-					except KeyError:
-						pass
-					except CacheError:
+					except (KeyError, CacheError):
 						pass
-			else:
-				eapi = metadata.get('EAPI', '').strip()
-				if not eapi:
-					eapi = '0'
-				if not (eapi[:1] == '-' and eapi_is_supported(eapi[1:])) and \
-					emtime == metadata['_mtime_'] and \
-					eclass_db.is_eclass_data_valid(metadata['_eclasses_']):
-					doregen = False
-
-			if not doregen:
+				continue
+			eapi = metadata.get('EAPI', '').strip()
+			if not eapi:
+				eapi = '0'
+			if eapi[:1] == '-' and eapi_is_supported(eapi[1:]):
+				continue
+			if auxdb.validate_entry(metadata, ebuild_hash, eclass_db):
 				break
-
-		if doregen:
+		else:
 			metadata = None
 
-		return (metadata, st, emtime)
+		return (metadata, ebuild_hash)
 
 	def aux_get(self, mycpv, mylist, mytree=None, myrepo=None):
 		"stub code for returning auxilliary db information, such as SLOT, DEPEND, etc."
@@ -467,7 +469,7 @@ class portdbapi(dbapi):
 				_("ebuild not found for '%s'") % mycpv, noiselevel=1)
 			raise KeyError(mycpv)
 
-		mydata, st, emtime = self._pull_valid_cache(mycpv, myebuild, mylocation)
+		mydata, ebuild_hash = self._pull_valid_cache(mycpv, myebuild, mylocation)
 		doregen = mydata is None
 
 		if doregen:
@@ -490,10 +492,10 @@ class portdbapi(dbapi):
 
 			if eapi is not None and not portage.eapi_is_supported(eapi):
 				mydata = self._metadata_callback(
-					mycpv, myebuild, mylocation, {'EAPI':eapi}, emtime)
+					mycpv, mylocation, {'EAPI':eapi}, ebuild_hash)
 			else:
-				proc = EbuildMetadataPhase(cpv=mycpv, ebuild_path=myebuild,
-					ebuild_mtime=emtime,
+				proc = EbuildMetadataPhase(cpv=mycpv,
+					ebuild_hash=ebuild_hash,
 					metadata_callback=self._metadata_callback, portdb=self,
 					repo_path=mylocation,
 					scheduler=PollScheduler().sched_iface,
@@ -511,15 +513,17 @@ class portdbapi(dbapi):
 		# do we have a origin repository name for the current package
 		mydata["repository"] = self.repositories.get_name_for_location(mylocation)
 		mydata["INHERITED"] = ' '.join(mydata.get("_eclasses_", []))
-		mydata["_mtime_"] = st[stat.ST_MTIME]
+		mydata["_mtime_"] = ebuild_hash.mtime
 
 		eapi = mydata.get("EAPI")
 		if not eapi:
 			eapi = "0"
 			mydata["EAPI"] = eapi
 		if not eapi_is_supported(eapi):
-			for k in set(mydata).difference(("_mtime_", "_eclasses_")):
-				mydata[k] = ""
+			keys = set(mydata)
+			keys.discard("_eclasses_")
+			keys.discard("_mtime_")
+			mydata.update((k, '') for k in keys)
 			mydata["EAPI"] = "-" + eapi.lstrip("-")
 
 		#finally, we look at our internal cache entry and return the requested data.
@@ -576,7 +580,9 @@ class portdbapi(dbapi):
 		if myebuild is None:
 			raise AssertionError(_("ebuild not found for '%s'") % mypkg)
 		pkgdir = os.path.dirname(myebuild)
-		mf = Manifest(pkgdir, self.settings["DISTDIR"])
+		mf = self.repositories.get_repo_for_location(
+			os.path.dirname(os.path.dirname(pkgdir))).load_manifest(
+				pkgdir, self.settings["DISTDIR"])
 		checksums = mf.getDigests()
 		if not checksums:
 			if debug: 
@@ -644,7 +650,9 @@ class portdbapi(dbapi):
 		if myebuild is None:
 			raise AssertionError(_("ebuild not found for '%s'") % mypkg)
 		pkgdir = os.path.dirname(myebuild)
-		mf = Manifest(pkgdir, self.settings["DISTDIR"])
+		mf = self.repositories.get_repo_for_location(
+			os.path.dirname(os.path.dirname(pkgdir)))
+		mf = mf.load_manifest(pkgdir, self.settings["DISTDIR"])
 		mysums = mf.getDigests()
 
 		failures = {}
diff --git a/pym/portage/dbapi/vartree.py b/pym/portage/dbapi/vartree.py
index 7f7873b..5214aa1 100644
--- a/pym/portage/dbapi/vartree.py
+++ b/pym/portage/dbapi/vartree.py
@@ -60,6 +60,7 @@ from _emerge.PollScheduler import PollScheduler
 from _emerge.MiscFunctionsProcess import MiscFunctionsProcess
 
 import errno
+import fileinput
 import gc
 import io
 from itertools import chain
@@ -194,7 +195,7 @@ class vardbapi(dbapi):
 		"""
 		if self._lock_count:
 			self._lock_count += 1
-		else:
+		elif os.environ.get("PORTAGE_LOCKS") != "false":
 			if self._lock is not None:
 				raise AssertionError("already locked")
 			# At least the parent needs to exist for the lock file.
@@ -210,7 +211,7 @@ class vardbapi(dbapi):
 		"""
 		if self._lock_count > 1:
 			self._lock_count -= 1
-		else:
+		elif os.environ.get("PORTAGE_LOCKS") != "false":
 			if self._lock is None:
 				raise AssertionError("not locked")
 			self._lock_count = 0
@@ -1839,16 +1840,10 @@ class dblink(object):
 		else:
 			self.settings.pop("PORTAGE_LOG_FILE", None)
 
-		# Lock the config memory file to prevent symlink creation
-		# in merge_contents from overlapping with env-update.
-		self.vartree.dbapi._fs_lock()
-		try:
-			env_update(target_root=self.settings['ROOT'],
-				prev_mtimes=ldpath_mtimes,
-				contents=contents, env=self.settings.environ(),
-				writemsg_level=self._display_merge)
-		finally:
-			self.vartree.dbapi._fs_unlock()
+		env_update(target_root=self.settings['ROOT'],
+			prev_mtimes=ldpath_mtimes,
+			contents=contents, env=self.settings.environ(),
+			writemsg_level=self._display_merge, vardbapi=self.vartree.dbapi)
 
 		return os.EX_OK
 
@@ -3270,6 +3265,15 @@ class dblink(object):
 					max_dblnk = dblnk
 			self._installed_instance = max_dblnk
 
+		if self.settings.get("INSTALL_MASK"):
+			# Apply INSTALL_MASK before collision-protect, since it may
+			# be useful to avoid collisions in some scenarios.
+			phase = MiscFunctionsProcess(background=False,
+				commands=["preinst_mask"], phase="preinst",
+				scheduler=self._scheduler, settings=self.settings)
+			phase.start()
+			phase.wait()
+
 		# We check for unicode encoding issues after src_install. However,
 		# the check must be repeated here for binary packages (it's
 		# inexpensive since we call os.walk() here anyway).
@@ -3394,14 +3398,6 @@ class dblink(object):
 			if installed_files:
 				return 1
 
-		# check for package collisions
-		blockers = self._blockers
-		if blockers is None:
-			blockers = []
-		collisions, symlink_collisions, plib_collisions = \
-			self._collision_protect(srcroot, destroot,
-			others_in_slot + blockers, myfilelist, mylinklist)
-
 		# Make sure the ebuild environment is initialized and that ${T}/elog
 		# exists for logging of collision-protect eerror messages.
 		if myebuild is None:
@@ -3413,6 +3409,14 @@ class dblink(object):
 			for other in others_in_slot])
 		prepare_build_dirs(settings=self.settings, cleanup=cleanup)
 
+		# check for package collisions
+		blockers = self._blockers
+		if blockers is None:
+			blockers = []
+		collisions, symlink_collisions, plib_collisions = \
+			self._collision_protect(srcroot, destroot,
+			others_in_slot + blockers, myfilelist, mylinklist)
+
 		if collisions:
 			collision_protect = "collision-protect" in self.settings.features
 			protect_owned = "protect-owned" in self.settings.features
@@ -3803,17 +3807,24 @@ class dblink(object):
 			if pkgcmp(catpkgsplit(self.pkg)[1:], catpkgsplit(v)[1:]) < 0:
 				downgrade = True
 
-		# Lock the config memory file to prevent symlink creation
-		# in merge_contents from overlapping with env-update.
-		self.vartree.dbapi._fs_lock()
-		try:
-			#update environment settings, library paths. DO NOT change symlinks.
-			env_update(makelinks=(not downgrade),
-				target_root=self.settings['ROOT'], prev_mtimes=prev_mtimes,
-				contents=contents, env=self.settings.environ(),
-				writemsg_level=self._display_merge)
-		finally:
-			self.vartree.dbapi._fs_unlock()
+		#update environment settings, library paths. DO NOT change symlinks.
+		env_update(makelinks=(not downgrade),
+			target_root=self.settings['ROOT'], prev_mtimes=prev_mtimes,
+			contents=contents, env=self.settings.environ(),
+			writemsg_level=self._display_merge, vardbapi=self.vartree.dbapi)
+
+		# Fix *.la files to point to libs in target_root, if they
+		# don't do so already.
+		re_root = self.settings["ROOT"].strip("/")
+		if re_root:
+			fix_files = []
+			for path in contents:
+				if path.endswith(".la"):
+					if os.path.exists(path): fix_files.append(path)
+			if fix_files:
+				pat = re.compile(r"([' =](?:-[IL])?/)(usr|lib|opt)")
+				for line in fileinput.input(fix_files, inplace=1):
+					sys.stdout.write(pat.sub(r"\1%s/\2" % re_root, line))
 
 		# For gcc upgrades, preserved libs have to be removed after the
 		# the library path has been updated.
diff --git a/pym/portage/eclass_cache.py b/pym/portage/eclass_cache.py
index 1374f1d..1044ad0 100644
--- a/pym/portage/eclass_cache.py
+++ b/pym/portage/eclass_cache.py
@@ -6,21 +6,57 @@ __all__ = ["cache"]
 
 import stat
 import sys
+import operator
 from portage.util import normalize_path
 import errno
-from portage.exception import PermissionDenied
+from portage.exception import FileNotFound, PermissionDenied
 from portage import os
+from portage import checksum
 
 if sys.hexversion >= 0x3000000:
 	long = int
 
+
+class hashed_path(object):
+
+	def __init__(self, location):
+		self.location = location
+
+	def __getattr__(self, attr):
+		if attr == 'mtime':
+			# use stat.ST_MTIME; accessing .st_mtime gets you a float
+			# depending on the python version, and long(float) introduces
+			# some rounding issues that aren't present for people using
+			# the straight c api.
+			# thus use the defacto python compatibility work around;
+			# access via index, which guarantees you get the raw long.
+			try:
+				self.mtime = obj = os.stat(self.location)[stat.ST_MTIME]
+			except OSError as e:
+				if e.errno in (errno.ENOENT, errno.ESTALE):
+					raise FileNotFound(self.location)
+				elif e.errno == PermissionDenied.errno:
+					raise PermissionDenied(self.location)
+				raise
+			return obj
+		if not attr.islower():
+			# we don't care to allow .mD5 as an alias for .md5
+			raise AttributeError(attr)
+		hashname = attr.upper()
+		if hashname not in checksum.hashfunc_map:
+			raise AttributeError(attr)
+		val = checksum.perform_checksum(self.location, hashname)[0]
+		setattr(self, attr, val)
+		return val
+
+
 class cache(object):
 	"""
 	Maintains the cache information about eclasses used in ebuild.
 	"""
 	def __init__(self, porttree_root, overlays=[]):
 
-		self.eclasses = {} # {"Name": ("location","_mtime_")}
+		self.eclasses = {} # {"Name": hashed_path}
 		self._eclass_locations = {}
 
 		# screw with the porttree ordering, w/out having bash inherit match it, and I'll hurt you.
@@ -80,14 +116,16 @@ class cache(object):
 			for y in eclass_filenames:
 				if not y.endswith(".eclass"):
 					continue
+				obj = hashed_path(os.path.join(x, y))
+				obj.eclass_dir = x
 				try:
-					mtime = os.stat(os.path.join(x, y))[stat.ST_MTIME]
-				except OSError:
+					mtime = obj.mtime
+				except FileNotFound:
 					continue
 				ys=y[:-eclass_len]
 				if x == self._master_eclass_root:
 					master_eclasses[ys] = mtime
-					self.eclasses[ys] = (x, mtime)
+					self.eclasses[ys] = obj
 					self._eclass_locations[ys] = x
 					continue
 
@@ -98,22 +136,30 @@ class cache(object):
 						# so prefer the master entry.
 						continue
 
-				self.eclasses[ys] = (x, mtime)
+				self.eclasses[ys] = obj
 				self._eclass_locations[ys] = x
 
-	def is_eclass_data_valid(self, ec_dict):
+	def validate_and_rewrite_cache(self, ec_dict, chf_type, stores_paths):
+		"""
+		This will return an empty dict if the ec_dict parameter happens
+		to be empty, therefore callers must take care to distinguish
+		between empty dict and None return values.
+		"""
 		if not isinstance(ec_dict, dict):
-			return False
-		for eclass, tup in ec_dict.items():
-			cached_data = self.eclasses.get(eclass, None)
-			""" Only use the mtime for validation since the probability of a
-			collision is small and, depending on the cache implementation, the
-			path may not be specified (cache from rsync mirrors, for example).
-			"""
-			if cached_data is None or tup[1] != cached_data[1]:
-				return False
-
-		return True
+			return None
+		our_getter = operator.attrgetter(chf_type)
+		cache_getter = lambda x:x
+		if stores_paths:
+			cache_getter = operator.itemgetter(1)
+		d = {}
+		for eclass, ec_data in ec_dict.items():
+			cached_data = self.eclasses.get(eclass)
+			if cached_data is None:
+				return None
+			if cache_getter(ec_data) != our_getter(cached_data):
+				return None
+			d[eclass] = cached_data
+		return d
 
 	def get_eclass_data(self, inherits):
 		ec_dict = {}
diff --git a/pym/portage/manifest.py b/pym/portage/manifest.py
index 13efab7..b2f96e6 100644
--- a/pym/portage/manifest.py
+++ b/pym/portage/manifest.py
@@ -49,6 +49,12 @@ def guessManifestFileType(filename):
 	else:
 		return "DIST"
 
+def guessThinManifestFileType(filename):
+	type = guessManifestFileType(filename)
+	if type != "DIST":
+		return None
+	return "DIST"
+
 def parseManifest2(mysplit):
 	myentry = None
 	if len(mysplit) > 4 and mysplit[0] in portage.const.MANIFEST2_IDENTIFIERS:
@@ -93,12 +99,15 @@ class Manifest2Entry(ManifestEntry):
 class Manifest(object):
 	parsers = (parseManifest2,)
 	def __init__(self, pkgdir, distdir, fetchlist_dict=None,
-		manifest1_compat=False, from_scratch=False):
+		manifest1_compat=False, from_scratch=False, thin=False, allow_missing=False,
+		allow_create=True):
 		""" create new Manifest instance for package in pkgdir
 		    and add compability entries for old portage versions if manifest1_compat == True.
 		    Do not parse Manifest file if from_scratch == True (only for internal use)
 			The fetchlist_dict parameter is required only for generation of
-			a Manifest (not needed for parsing and checking sums)."""
+			a Manifest (not needed for parsing and checking sums).
+			If thin is specified, then the manifest carries only info for
+			distfiles."""
 		self.pkgdir = _unicode_decode(pkgdir).rstrip(os.sep) + os.sep
 		self.fhashdict = {}
 		self.hashes = set()
@@ -120,7 +129,13 @@ class Manifest(object):
 		else:
 			self.fetchlist_dict = {}
 		self.distdir = distdir
-		self.guessType = guessManifestFileType
+		self.thin = thin
+		if thin:
+			self.guessType = guessThinManifestFileType
+		else:
+			self.guessType = guessManifestFileType
+		self.allow_missing = allow_missing
+		self.allow_create = allow_create
 
 	def getFullname(self):
 		""" Returns the absolute path to the Manifest file for this instance """
@@ -223,11 +238,13 @@ class Manifest(object):
 
 	def write(self, sign=False, force=False):
 		""" Write Manifest instance to disk, optionally signing it """
+		if not self.allow_create:
+			return
 		self.checkIntegrity()
 		try:
 			myentries = list(self._createManifestEntries())
 			update_manifest = True
-			if not force:
+			if myentries and not force:
 				try:
 					f = io.open(_unicode_encode(self.getFullname(),
 						encoding=_encodings['fs'], errors='strict'),
@@ -246,9 +263,20 @@ class Manifest(object):
 						pass
 					else:
 						raise
+
 			if update_manifest:
-				write_atomic(self.getFullname(),
-					"".join("%s\n" % str(myentry) for myentry in myentries))
+				if myentries or not (self.thin or self.allow_missing):
+					write_atomic(self.getFullname(), "".join("%s\n" %
+						str(myentry) for myentry in myentries))
+				else:
+					# With thin manifest, there's no need to have
+					# a Manifest file if there are no DIST entries.
+					try:
+						os.unlink(self.getFullname())
+					except OSError as e:
+						if e.errno != errno.ENOENT:
+							raise
+
 			if sign:
 				self.sign()
 		except (IOError, OSError) as e:
@@ -305,6 +333,8 @@ class Manifest(object):
 		distfiles to raise a FileNotFound exception for (if no file or existing
 		checksums are available), and defaults to all distfiles when not
 		specified."""
+		if not self.allow_create:
+			return
 		if checkExisting:
 			self.checkAllHashes()
 		if assumeDistHashesSometimes or assumeDistHashesAlways:
@@ -313,64 +343,20 @@ class Manifest(object):
 			distfilehashes = {}
 		self.__init__(self.pkgdir, self.distdir,
 			fetchlist_dict=self.fetchlist_dict, from_scratch=True,
-			manifest1_compat=False)
-		cpvlist = []
+			manifest1_compat=False, thin=self.thin)
 		pn = os.path.basename(self.pkgdir.rstrip(os.path.sep))
 		cat = self._pkgdir_category()
 
 		pkgdir = self.pkgdir
+		if self.thin:
+			cpvlist = self._update_thin_pkgdir(cat, pn, pkgdir)
+		else:
+			cpvlist = self._update_thick_pkgdir(cat, pn, pkgdir)
 
-		for pkgdir, pkgdir_dirs, pkgdir_files in os.walk(pkgdir):
-			break
-		for f in pkgdir_files:
-			try:
-				f = _unicode_decode(f,
-					encoding=_encodings['fs'], errors='strict')
-			except UnicodeDecodeError:
-				continue
-			if f[:1] == ".":
-				continue
-			pf = None
-			if f[-7:] == '.ebuild':
-				pf = f[:-7]
-			if pf is not None:
-				mytype = "EBUILD"
-				ps = portage.versions._pkgsplit(pf)
-				cpv = "%s/%s" % (cat, pf)
-				if not ps:
-					raise PortagePackageException(
-						_("Invalid package name: '%s'") % cpv)
-				if ps[0] != pn:
-					raise PortagePackageException(
-						_("Package name does not "
-						"match directory name: '%s'") % cpv)
-				cpvlist.append(cpv)
-			elif manifest2MiscfileFilter(f):
-				mytype = "MISC"
-			else:
-				continue
-			self.fhashdict[mytype][f] = perform_multiple_checksums(self.pkgdir+f, self.hashes)
-		recursive_files = []
-
-		pkgdir = self.pkgdir
-		cut_len = len(os.path.join(pkgdir, "files") + os.sep)
-		for parentdir, dirs, files in os.walk(os.path.join(pkgdir, "files")):
-			for f in files:
-				try:
-					f = _unicode_decode(f,
-						encoding=_encodings['fs'], errors='strict')
-				except UnicodeDecodeError:
-					continue
-				full_path = os.path.join(parentdir, f)
-				recursive_files.append(full_path[cut_len:])
-		for f in recursive_files:
-			if not manifest2AuxfileFilter(f):
-				continue
-			self.fhashdict["AUX"][f] = perform_multiple_checksums(
-				os.path.join(self.pkgdir, "files", f.lstrip(os.sep)), self.hashes)
 		distlist = set()
 		for cpv in cpvlist:
 			distlist.update(self._getCpvDistfiles(cpv))
+
 		if requiredDistfiles is None:
 			# This allows us to force removal of stale digests for the
 			# ebuild --force digest option (no distfiles are required).
@@ -404,6 +390,81 @@ class Manifest(object):
 					if f in requiredDistfiles:
 						raise
 
+	def _is_cpv(self, cat, pn, filename):
+		if not filename.endswith(".ebuild"):
+			return None
+		pf = filename[:-7]
+		if pf is None:
+			return None
+		ps = portage.versions._pkgsplit(pf)
+		cpv = "%s/%s" % (cat, pf)
+		if not ps:
+			raise PortagePackageException(
+				_("Invalid package name: '%s'") % cpv)
+		if ps[0] != pn:
+			raise PortagePackageException(
+				_("Package name does not "
+				"match directory name: '%s'") % cpv)
+		return cpv
+
+	def _update_thin_pkgdir(self, cat, pn, pkgdir):
+		for pkgdir, pkgdir_dirs, pkgdir_files in os.walk(pkgdir):
+			break
+		cpvlist = []
+		for f in pkgdir_files:
+			try:
+				f = _unicode_decode(f,
+					encoding=_encodings['fs'], errors='strict')
+			except UnicodeDecodeError:
+				continue
+			if f[:1] == '.':
+				continue
+			pf = self._is_cpv(cat, pn, f)
+			if pf is not None:
+				cpvlist.append(pf)
+		return cpvlist
+
+	def _update_thick_pkgdir(self, cat, pn, pkgdir):
+		cpvlist = []
+		for pkgdir, pkgdir_dirs, pkgdir_files in os.walk(pkgdir):
+			break
+		for f in pkgdir_files:
+			try:
+				f = _unicode_decode(f,
+					encoding=_encodings['fs'], errors='strict')
+			except UnicodeDecodeError:
+				continue
+			if f[:1] == ".":
+				continue
+			pf = self._is_cpv(cat, pn, f)
+			if pf is not None:
+				mytype = "EBUILD"
+				cpvlist.append(pf)
+			elif manifest2MiscfileFilter(f):
+				mytype = "MISC"
+			else:
+				continue
+			self.fhashdict[mytype][f] = perform_multiple_checksums(self.pkgdir+f, self.hashes)
+		recursive_files = []
+
+		pkgdir = self.pkgdir
+		cut_len = len(os.path.join(pkgdir, "files") + os.sep)
+		for parentdir, dirs, files in os.walk(os.path.join(pkgdir, "files")):
+			for f in files:
+				try:
+					f = _unicode_decode(f,
+						encoding=_encodings['fs'], errors='strict')
+				except UnicodeDecodeError:
+					continue
+				full_path = os.path.join(parentdir, f)
+				recursive_files.append(full_path[cut_len:])
+		for f in recursive_files:
+			if not manifest2AuxfileFilter(f):
+				continue
+			self.fhashdict["AUX"][f] = perform_multiple_checksums(
+				os.path.join(self.pkgdir, "files", f.lstrip(os.sep)), self.hashes)
+		return cpvlist
+
 	def _pkgdir_category(self):
 		return self.pkgdir.rstrip(os.sep).split(os.sep)[-2]
 
diff --git a/pym/portage/package/ebuild/_config/special_env_vars.py b/pym/portage/package/ebuild/_config/special_env_vars.py
index 87aa606..02a7844 100644
--- a/pym/portage/package/ebuild/_config/special_env_vars.py
+++ b/pym/portage/package/ebuild/_config/special_env_vars.py
@@ -156,7 +156,7 @@ environ_filter += [
 	"PORTAGE_RO_DISTDIRS",
 	"PORTAGE_RSYNC_EXTRA_OPTS", "PORTAGE_RSYNC_OPTS",
 	"PORTAGE_RSYNC_RETRIES", "PORTAGE_SYNC_STALE",
-	"PORTAGE_USE", "PORT_LOGDIR",
+	"PORTAGE_USE", "PORT_LOGDIR", "PORT_LOGDIR_CLEAN",
 	"QUICKPKG_DEFAULT_OPTS",
 	"RESUMECOMMAND", "RESUMECOMMAND_FTP",
 	"RESUMECOMMAND_HTTP", "RESUMECOMMAND_HTTPS",
diff --git a/pym/portage/package/ebuild/config.py b/pym/portage/package/ebuild/config.py
index a591c9a..5ed477e 100644
--- a/pym/portage/package/ebuild/config.py
+++ b/pym/portage/package/ebuild/config.py
@@ -309,7 +309,6 @@ class config(object):
 			if self.modules["user"] is None:
 				self.modules["user"] = {}
 			self.modules["default"] = {
-				"portdbapi.metadbmodule": "portage.cache.metadata.database",
 				"portdbapi.auxdbmodule":  "portage.cache.flat_hash.database",
 			}
 
@@ -864,7 +863,11 @@ class config(object):
 				try:
 					mod = load_mod(best_mod)
 				except ImportError:
-					raise
+					if best_mod == "portage.cache.metadata_overlay.database":
+						best_mod = "portage.cache.flat_hash.database"
+						mod = load_mod(best_mod)
+					else:
+						raise
 		return mod
 
 	def lock(self):
diff --git a/pym/portage/package/ebuild/digestcheck.py b/pym/portage/package/ebuild/digestcheck.py
index 1e34b14..6c823e0 100644
--- a/pym/portage/package/ebuild/digestcheck.py
+++ b/pym/portage/package/ebuild/digestcheck.py
@@ -28,49 +28,33 @@ def digestcheck(myfiles, mysettings, strict=False, justmanifest=None, mf=None):
 
 	if mysettings.get("EBUILD_SKIP_MANIFEST") == "1":
 		return 1
-	allow_missing = "allow-missing-manifests" in mysettings.features
 	pkgdir = mysettings["O"]
-	manifest_path = os.path.join(pkgdir, "Manifest")
-	if not os.path.exists(manifest_path):
-		if allow_missing:
-			return 1
-		writemsg(_("!!! Manifest file not found: '%s'\n") % manifest_path,
-			noiselevel=-1)
-		if strict:
-			return 0
-		else:
-			return 1
 	if mf is None:
-		mf = Manifest(pkgdir, mysettings["DISTDIR"])
-	manifest_empty = True
-	for d in mf.fhashdict.values():
-		if d:
-			manifest_empty = False
-			break
-	if manifest_empty:
-		writemsg(_("!!! Manifest is empty: '%s'\n") % manifest_path,
-			noiselevel=-1)
-		if strict:
-			return 0
-		else:
-			return 1
+		mf = mysettings.repositories.get_repo_for_location(
+			os.path.dirname(os.path.dirname(pkgdir)))
+		mf = mf.load_manifest(pkgdir, mysettings["DISTDIR"])
 	eout = EOutput()
 	eout.quiet = mysettings.get("PORTAGE_QUIET", None) == "1"
 	try:
 		if strict and "PORTAGE_PARALLEL_FETCHONLY" not in mysettings:
-			eout.ebegin(_("checking ebuild checksums ;-)"))
-			mf.checkTypeHashes("EBUILD")
-			eout.eend(0)
-			eout.ebegin(_("checking auxfile checksums ;-)"))
-			mf.checkTypeHashes("AUX")
-			eout.eend(0)
-			eout.ebegin(_("checking miscfile checksums ;-)"))
-			mf.checkTypeHashes("MISC", ignoreMissingFiles=True)
-			eout.eend(0)
+			if mf.fhashdict.get("EBUILD"):
+				eout.ebegin(_("checking ebuild checksums ;-)"))
+				mf.checkTypeHashes("EBUILD")
+				eout.eend(0)
+			if mf.fhashdict.get("AUX"):
+				eout.ebegin(_("checking auxfile checksums ;-)"))
+				mf.checkTypeHashes("AUX")
+				eout.eend(0)
+			if mf.fhashdict.get("MISC"):
+				eout.ebegin(_("checking miscfile checksums ;-)"))
+				mf.checkTypeHashes("MISC", ignoreMissingFiles=True)
+				eout.eend(0)
 		for f in myfiles:
 			eout.ebegin(_("checking %s ;-)") % f)
 			ftype = mf.findFile(f)
 			if ftype is None:
+				if mf.allow_missing:
+					continue
 				eout.eend(1)
 				writemsg(_("\n!!! Missing digest for '%s'\n") % (f,),
 					noiselevel=-1)
@@ -90,7 +74,7 @@ def digestcheck(myfiles, mysettings, strict=False, justmanifest=None, mf=None):
 		writemsg(_("!!! Got: %s\n") % e.value[2], noiselevel=-1)
 		writemsg(_("!!! Expected: %s\n") % e.value[3], noiselevel=-1)
 		return 0
-	if allow_missing:
+	if mf.thin or mf.allow_missing:
 		# In this case we ignore any missing digests that
 		# would otherwise be detected below.
 		return 1
diff --git a/pym/portage/package/ebuild/digestgen.py b/pym/portage/package/ebuild/digestgen.py
index eb7210e..f7cf149 100644
--- a/pym/portage/package/ebuild/digestgen.py
+++ b/pym/portage/package/ebuild/digestgen.py
@@ -53,8 +53,15 @@ def digestgen(myarchives=None, mysettings=None, myportdb=None):
 				return 0
 		mytree = os.path.dirname(os.path.dirname(mysettings["O"]))
 		manifest1_compat = False
-		mf = Manifest(mysettings["O"], mysettings["DISTDIR"],
+		mf = mysettings.repositories.get_repo_for_location(mytree)
+		mf = mf.load_manifest(mysettings["O"], mysettings["DISTDIR"],
 			fetchlist_dict=fetchlist_dict, manifest1_compat=manifest1_compat)
+
+		if not mf.allow_create:
+			writemsg_stdout(_(">>> Skipping creating Manifest for %s; "
+				"repository is configured to not use them\n") % mysettings["O"])
+			return 1
+
 		# Don't require all hashes since that can trigger excessive
 		# fetches when sufficient digests already exist.  To ease transition
 		# while Manifest 1 is being removed, only require hashes that will
diff --git a/pym/portage/package/ebuild/doebuild.py b/pym/portage/package/ebuild/doebuild.py
index a710e09..f5c4c81 100644
--- a/pym/portage/package/ebuild/doebuild.py
+++ b/pym/portage/package/ebuild/doebuild.py
@@ -3,6 +3,7 @@
 
 __all__ = ['doebuild', 'doebuild_environment', 'spawn', 'spawnebuild']
 
+import fileinput
 import gzip
 import errno
 import io
@@ -50,7 +51,6 @@ from portage.exception import DigestException, FileNotFound, \
 	IncorrectParameter, InvalidDependString, PermissionDenied, \
 	UnsupportedAPIException
 from portage.localization import _
-from portage.manifest import Manifest
 from portage.output import style_to_ansi_code
 from portage.package.ebuild.prepare_build_dirs import prepare_build_dirs
 from portage.util import apply_recursive_permissions, \
@@ -480,21 +480,27 @@ def doebuild(myebuild, mydo, myroot, mysettings, debug=0, listonly=0,
 		return 1
 
 	global _doebuild_manifest_cache
+	pkgdir = os.path.dirname(myebuild)
+	manifest_path = os.path.join(pkgdir, "Manifest")
+	if tree == "porttree":
+		repo_config = mysettings.repositories.get_repo_for_location(
+			os.path.dirname(os.path.dirname(pkgdir)))
+	else:
+		repo_config = None
 	mf = None
 	if "strict" in features and \
 		"digest" not in features and \
 		tree == "porttree" and \
+		not repo_config.thin_manifest and \
 		mydo not in ("digest", "manifest", "help") and \
-		not portage._doebuild_manifest_exempt_depend:
+		not portage._doebuild_manifest_exempt_depend and \
+		not (repo_config.allow_missing_manifest and not os.path.exists(manifest_path)):
 		# Always verify the ebuild checksums before executing it.
 		global _doebuild_broken_ebuilds
 
 		if myebuild in _doebuild_broken_ebuilds:
 			return 1
 
-		pkgdir = os.path.dirname(myebuild)
-		manifest_path = os.path.join(pkgdir, "Manifest")
-
 		# Avoid checking the same Manifest several times in a row during a
 		# regen with an empty cache.
 		if _doebuild_manifest_cache is None or \
@@ -505,7 +511,7 @@ def doebuild(myebuild, mydo, myroot, mysettings, debug=0, listonly=0,
 				out.eerror(_("Manifest not found for '%s'") % (myebuild,))
 				_doebuild_broken_ebuilds.add(myebuild)
 				return 1
-			mf = Manifest(pkgdir, mysettings["DISTDIR"])
+			mf = repo_config.load_manifest(pkgdir, mysettings["DISTDIR"])
 
 		else:
 			mf = _doebuild_manifest_cache
@@ -513,10 +519,12 @@ def doebuild(myebuild, mydo, myroot, mysettings, debug=0, listonly=0,
 		try:
 			mf.checkFileHashes("EBUILD", os.path.basename(myebuild))
 		except KeyError:
-			out = portage.output.EOutput()
-			out.eerror(_("Missing digest for '%s'") % (myebuild,))
-			_doebuild_broken_ebuilds.add(myebuild)
-			return 1
+			if not (mf.allow_missing and
+				os.path.basename(myebuild) not in mf.fhashdict["EBUILD"]):
+				out = portage.output.EOutput()
+				out.eerror(_("Missing digest for '%s'") % (myebuild,))
+				_doebuild_broken_ebuilds.add(myebuild)
+				return 1
 		except FileNotFound:
 			out = portage.output.EOutput()
 			out.eerror(_("A file listed in the Manifest "
@@ -536,7 +544,7 @@ def doebuild(myebuild, mydo, myroot, mysettings, debug=0, listonly=0,
 		if mf.getFullname() in _doebuild_broken_manifests:
 			return 1
 
-		if mf is not _doebuild_manifest_cache:
+		if mf is not _doebuild_manifest_cache and not mf.allow_missing:
 
 			# Make sure that all of the ebuilds are
 			# actually listed in the Manifest.
@@ -553,8 +561,8 @@ def doebuild(myebuild, mydo, myroot, mysettings, debug=0, listonly=0,
 					_doebuild_broken_manifests.add(manifest_path)
 					return 1
 
-			# Only cache it if the above stray files test succeeds.
-			_doebuild_manifest_cache = mf
+		# We cache it only after all above checks succeed.
+		_doebuild_manifest_cache = mf
 
 	logfile=None
 	builddir_lock = None
@@ -1300,13 +1308,14 @@ _post_phase_cmds = {
 
 	"install" : [
 		"install_qa_check",
-		"install_symlink_html_docs"],
+		"install_symlink_html_docs",
+		"install_hooks"],
 
 	"preinst" : [
 		"preinst_sfperms",
 		"preinst_selinux_labels",
 		"preinst_suid_scan",
-		"preinst_mask"]
+		]
 }
 
 def _post_phase_userpriv_perms(mysettings):
@@ -1497,6 +1506,7 @@ def _post_src_install_uid_fix(mysettings, out):
 
 	destdir = mysettings["D"]
 	unicode_errors = []
+	fix_files = []
 
 	while True:
 
@@ -1585,10 +1595,12 @@ def _post_src_install_uid_fix(mysettings, out):
 							new_contents, mode='wb')
 
 				mystat = os.lstat(fpath)
-				if stat.S_ISREG(mystat.st_mode) and \
-					mystat.st_ino not in counted_inodes:
-					counted_inodes.add(mystat.st_ino)
-					size += mystat.st_size
+				if stat.S_ISREG(mystat.st_mode):
+					if fname.endswith(".la"):
+						fix_files.append(fpath)
+					if mystat.st_ino not in counted_inodes:
+						counted_inodes.add(mystat.st_ino)
+						size += mystat.st_size
 				if mystat.st_uid != portage_uid and \
 					mystat.st_gid != portage_gid:
 					continue
@@ -1656,6 +1668,14 @@ def _post_src_install_uid_fix(mysettings, out):
 			mode='w', encoding=_encodings['repo.content'],
 			errors='strict').write(_unicode_decode(v + '\n'))
 
+	re_root = mysettings["ROOT"].strip("/")
+	if fix_files and re_root:
+		# Replace references to our sysroot with references to "/" in binpkg.
+		# Sysroot will be re-appended when the package is installed.
+		pat = re.compile(r"([' =](-[IL])?/)%s/" % re.escape(re_root))
+		for line in fileinput.input(fix_files, inplace=1):
+			sys.stdout.write(pat.sub(r"\1", line))
+
 	_reapply_bsdflags_to_image(mysettings)
 
 def _reapply_bsdflags_to_image(mysettings):
diff --git a/pym/portage/package/ebuild/fetch.py b/pym/portage/package/ebuild/fetch.py
index 5cbbf87..11c4c01 100644
--- a/pym/portage/package/ebuild/fetch.py
+++ b/pym/portage/package/ebuild/fetch.py
@@ -356,7 +356,8 @@ def fetch(myuris, mysettings, listonly=0, fetchonly=0,
 		allow_missing_digests = True
 	pkgdir = mysettings.get("O")
 	if digests is None and not (pkgdir is None or skip_manifest):
-		mydigests = Manifest(
+		mydigests = mysettings.repositories.get_repo_for_location(
+			os.path.dirname(os.path.dirname(pkgdir))).load_manifest(
 			pkgdir, mysettings["DISTDIR"]).getTypeDigests("DIST")
 	elif digests is None or skip_manifest:
 		# no digests because fetch was not called for a specific package
diff --git a/pym/portage/repository/config.py b/pym/portage/repository/config.py
index 9f0bb99..0fe4a0e 100644
--- a/pym/portage/repository/config.py
+++ b/pym/portage/repository/config.py
@@ -16,6 +16,7 @@ from portage.util import normalize_path, writemsg, writemsg_level, shlex_split
 from portage.localization import _
 from portage import _unicode_encode
 from portage import _encodings
+from portage import manifest
 
 _repo_name_sub_re = re.compile(r'[^\w-]')
 
@@ -36,7 +37,8 @@ class RepoConfig(object):
 	"""Stores config of one repository"""
 
 	__slots__ = ['aliases', 'eclass_overrides', 'eclass_locations', 'location', 'user_location', 'masters', 'main_repo',
-		'missing_repo_name', 'name', 'priority', 'sync', 'format']
+		'missing_repo_name', 'name', 'priority', 'sync', 'format', 'sign_manifest', 'thin_manifest',
+		'allow_missing_manifest', 'create_manifest', 'disable_manifest', 'cache_format']
 
 	def __init__(self, name, repo_opts):
 		"""Build a RepoConfig with options in repo_opts
@@ -105,6 +107,36 @@ class RepoConfig(object):
 			missing = False
 		self.name = name
 		self.missing_repo_name = missing
+		self.thin_manifest = False
+		self.allow_missing_manifest = False
+		self.create_manifest = True
+		self.disable_manifest = False
+		self.cache_format = None
+
+	def get_pregenerated_cache(self, auxdbkeys, readonly=True, force=False):
+		format = self.cache_format
+		if format is None:
+			if not force:
+				return None
+			format = 'pms'
+		if format == 'pms':
+			from portage.cache.metadata import database
+			name = 'metadata/cache'
+		elif format == 'md5-dict':
+			from portage.cache.flat_hash import md5_database as database
+			name = 'metadata/md5-cache'
+		else:
+			return None
+		return database(self.location, name,
+			auxdbkeys, readonly=readonly)
+
+	def load_manifest(self, *args, **kwds):
+		kwds['thin'] = self.thin_manifest
+		kwds['allow_missing'] = self.allow_missing_manifest
+		kwds['allow_create'] = self.create_manifest
+		if self.disable_manifest:
+			kwds['from_scratch'] = True
+		return manifest.Manifest(*args, **kwds)
 
 	def update(self, new_repo):
 		"""Update repository with options in another RepoConfig"""
@@ -169,107 +201,101 @@ class RepoConfig(object):
 
 class RepoConfigLoader(object):
 	"""Loads and store config of several repositories, loaded from PORTDIR_OVERLAY or repos.conf"""
-	def __init__(self, paths, settings):
-		"""Load config from files in paths"""
-		def parse(paths, prepos, ignored_map, ignored_location_map):
-			"""Parse files in paths to load config"""
-			parser = SafeConfigParser()
-			try:
-				parser.read(paths)
-			except ParsingError as e:
-				writemsg(_("!!! Error while reading repo config file: %s\n") % e, noiselevel=-1)
-			prepos['DEFAULT'] = RepoConfig("DEFAULT", parser.defaults())
-			for sname in parser.sections():
-				optdict = {}
-				for oname in parser.options(sname):
-					optdict[oname] = parser.get(sname, oname)
-
-				repo = RepoConfig(sname, optdict)
-				if repo.location and not os.path.exists(repo.location):
-					writemsg(_("!!! Invalid repos.conf entry '%s'"
-						" (not a dir): '%s'\n") % (sname, repo.location), noiselevel=-1)
-					continue
 
-				if repo.name in prepos:
-					old_location = prepos[repo.name].location
-					if old_location is not None and repo.location is not None and old_location != repo.location:
-						ignored_map.setdefault(repo.name, []).append(old_location)
-						ignored_location_map[old_location] = repo.name
-					prepos[repo.name].update(repo)
-				else:
-					prepos[repo.name] = repo
-
-		def add_overlays(portdir, portdir_overlay, prepos, ignored_map, ignored_location_map):
-			"""Add overlays in PORTDIR_OVERLAY as repositories"""
-			overlays = []
-			if portdir:
-				portdir = normalize_path(portdir)
-				overlays.append(portdir)
-			port_ov = [normalize_path(i) for i in shlex_split(portdir_overlay)]
-			overlays.extend(port_ov)
-			default_repo_opts = {}
-			if prepos['DEFAULT'].aliases is not None:
-				default_repo_opts['aliases'] = \
-					' '.join(prepos['DEFAULT'].aliases)
-			if prepos['DEFAULT'].eclass_overrides is not None:
-				default_repo_opts['eclass-overrides'] = \
-					' '.join(prepos['DEFAULT'].eclass_overrides)
-			if prepos['DEFAULT'].masters is not None:
-				default_repo_opts['masters'] = \
-					' '.join(prepos['DEFAULT'].masters)
-			if overlays:
-				#overlay priority is negative because we want them to be looked before any other repo
-				base_priority = 0
-				for ov in overlays:
-					if os.path.isdir(ov):
-						repo_opts = default_repo_opts.copy()
-						repo_opts['location'] = ov
-						repo = RepoConfig(None, repo_opts)
-						repo_conf_opts = prepos.get(repo.name)
-						if repo_conf_opts is not None:
-							if repo_conf_opts.aliases is not None:
-								repo_opts['aliases'] = \
-									' '.join(repo_conf_opts.aliases)
-							if repo_conf_opts.eclass_overrides is not None:
-								repo_opts['eclass-overrides'] = \
-									' '.join(repo_conf_opts.eclass_overrides)
-							if repo_conf_opts.masters is not None:
-								repo_opts['masters'] = \
-									' '.join(repo_conf_opts.masters)
-						repo = RepoConfig(repo.name, repo_opts)
-						if repo.name in prepos:
-							old_location = prepos[repo.name].location
-							if old_location is not None and old_location != repo.location:
-								ignored_map.setdefault(repo.name, []).append(old_location)
-								ignored_location_map[old_location] = repo.name
-								if old_location == portdir:
-									portdir = repo.user_location
-							prepos[repo.name].update(repo)
-							repo = prepos[repo.name]
-						else:
-							prepos[repo.name] = repo
-
-						if ov == portdir and portdir not in port_ov:
-							repo.priority = -1000
-						else:
-							repo.priority = base_priority
-							base_priority += 1
+	@staticmethod
+	def _add_overlays(portdir, portdir_overlay, prepos, ignored_map, ignored_location_map):
+		"""Add overlays in PORTDIR_OVERLAY as repositories"""
+		overlays = []
+		if portdir:
+			portdir = normalize_path(portdir)
+			overlays.append(portdir)
+		port_ov = [normalize_path(i) for i in shlex_split(portdir_overlay)]
+		overlays.extend(port_ov)
+		default_repo_opts = {}
+		if prepos['DEFAULT'].aliases is not None:
+			default_repo_opts['aliases'] = \
+				' '.join(prepos['DEFAULT'].aliases)
+		if prepos['DEFAULT'].eclass_overrides is not None:
+			default_repo_opts['eclass-overrides'] = \
+				' '.join(prepos['DEFAULT'].eclass_overrides)
+		if prepos['DEFAULT'].masters is not None:
+			default_repo_opts['masters'] = \
+				' '.join(prepos['DEFAULT'].masters)
+		if overlays:
+			#overlay priority is negative because we want them to be looked before any other repo
+			base_priority = 0
+			for ov in overlays:
+				if os.path.isdir(ov):
+					repo_opts = default_repo_opts.copy()
+					repo_opts['location'] = ov
+					repo = RepoConfig(None, repo_opts)
+					repo_conf_opts = prepos.get(repo.name)
+					if repo_conf_opts is not None:
+						if repo_conf_opts.aliases is not None:
+							repo_opts['aliases'] = \
+								' '.join(repo_conf_opts.aliases)
+						if repo_conf_opts.eclass_overrides is not None:
+							repo_opts['eclass-overrides'] = \
+								' '.join(repo_conf_opts.eclass_overrides)
+						if repo_conf_opts.masters is not None:
+							repo_opts['masters'] = \
+								' '.join(repo_conf_opts.masters)
+					repo = RepoConfig(repo.name, repo_opts)
+					if repo.name in prepos:
+						old_location = prepos[repo.name].location
+						if old_location is not None and old_location != repo.location:
+							ignored_map.setdefault(repo.name, []).append(old_location)
+							ignored_location_map[old_location] = repo.name
+							if old_location == portdir:
+								portdir = repo.user_location
+						prepos[repo.name].update(repo)
+						repo = prepos[repo.name]
+					else:
+						prepos[repo.name] = repo
 
+					if ov == portdir and portdir not in port_ov:
+						repo.priority = -1000
 					else:
-						writemsg(_("!!! Invalid PORTDIR_OVERLAY"
-							" (not a dir): '%s'\n") % ov, noiselevel=-1)
+						repo.priority = base_priority
+						base_priority += 1
+
+				else:
+					writemsg(_("!!! Invalid PORTDIR_OVERLAY"
+						" (not a dir): '%s'\n") % ov, noiselevel=-1)
+
+		return portdir
+
+	@staticmethod
+	def _parse(paths, prepos, ignored_map, ignored_location_map):
+		"""Parse files in paths to load config"""
+		parser = SafeConfigParser()
+		try:
+			parser.read(paths)
+		except ParsingError as e:
+			writemsg(_("!!! Error while reading repo config file: %s\n") % e, noiselevel=-1)
+		prepos['DEFAULT'] = RepoConfig("DEFAULT", parser.defaults())
+		for sname in parser.sections():
+			optdict = {}
+			for oname in parser.options(sname):
+				optdict[oname] = parser.get(sname, oname)
+
+			repo = RepoConfig(sname, optdict)
+			if repo.location and not os.path.exists(repo.location):
+				writemsg(_("!!! Invalid repos.conf entry '%s'"
+					" (not a dir): '%s'\n") % (sname, repo.location), noiselevel=-1)
+				continue
 
-			return portdir
+			if repo.name in prepos:
+				old_location = prepos[repo.name].location
+				if old_location is not None and repo.location is not None and old_location != repo.location:
+					ignored_map.setdefault(repo.name, []).append(old_location)
+					ignored_location_map[old_location] = repo.name
+				prepos[repo.name].update(repo)
+			else:
+				prepos[repo.name] = repo
 
-		def repo_priority(r):
-			"""
-			Key funtion for comparing repositories by priority.
-			None is equal priority zero.
-			"""
-			x = prepos[r].priority
-			if x is None:
-				return 0
-			return x
+	def __init__(self, paths, settings):
+		"""Load config from files in paths"""
 
 		prepos = {}
 		location_map = {}
@@ -279,10 +305,12 @@ class RepoConfigLoader(object):
 
 		portdir = settings.get('PORTDIR', '')
 		portdir_overlay = settings.get('PORTDIR_OVERLAY', '')
-		parse(paths, prepos, ignored_map, ignored_location_map)
+
+		self._parse(paths, prepos, ignored_map, ignored_location_map)
+
 		# If PORTDIR_OVERLAY contains a repo with the same repo_name as
 		# PORTDIR, then PORTDIR is overridden.
-		portdir = add_overlays(portdir, portdir_overlay, prepos,
+		portdir = self._add_overlays(portdir, portdir_overlay, prepos,
 			ignored_map, ignored_location_map)
 		if portdir and portdir.strip():
 			portdir = os.path.realpath(portdir)
@@ -319,6 +347,21 @@ class RepoConfigLoader(object):
 					aliases.extend(repo.aliases)
 				repo.aliases = tuple(sorted(set(aliases)))
 
+			if layout_data.get('thin-manifests', '').lower() == 'true':
+				repo.thin_manifest = True
+
+			manifest_policy = layout_data.get('use-manifests', 'strict').lower()
+			repo.allow_missing_manifest = manifest_policy != 'strict'
+			repo.create_manifest = manifest_policy != 'false'
+			repo.disable_manifest = manifest_policy == 'false'
+
+			# for compatibility w/ PMS, fallback to pms; but also check if the
+			# cache exists or not.
+			repo.cache_format = layout_data.get('cache-format', 'pms').lower()
+			if repo.cache_format == 'pms' and not os.path.isdir(
+				os.path.join(repo.location, 'metadata', 'cache')):
+				repo.cache_format = None
+
 		#Take aliases into account.
 		new_prepos = {}
 		for repo_name, repo in prepos.items():
@@ -342,9 +385,11 @@ class RepoConfigLoader(object):
 
 		# filter duplicates from aliases, by only including
 		# items where repo.name == key
-		prepos_order = [repo.name for key, repo in prepos.items() \
+
+		prepos_order = sorted(prepos.items(), key=lambda r:r[1].priority or 0)
+
+		prepos_order = [repo.name for (key, repo) in prepos_order
 			if repo.name == key and repo.location is not None]
-		prepos_order.sort(key=repo_priority)
 
 		if portdir in location_map:
 			portdir_repo = prepos[location_map[portdir]]
@@ -488,6 +533,9 @@ class RepoConfigLoader(object):
 			return None
 		return self.treemap[repo_name]
 
+	def get_repo_for_location(self, location):
+		return self.prepos[self.get_name_for_location(location)]
+
 	def __getitem__(self, repo_name):
 		return self.prepos[repo_name]
 
diff --git a/pym/portage/tests/resolver/test_rebuild.py b/pym/portage/tests/resolver/test_rebuild.py
index b9c4d6d..6f1a783 100644
--- a/pym/portage/tests/resolver/test_rebuild.py
+++ b/pym/portage/tests/resolver/test_rebuild.py
@@ -9,57 +9,58 @@ class RebuildTestCase(TestCase):
 
 	def testRebuild(self):
 		"""
-		Rebuild packages when dependencies that are used at both build-time and
-		run-time are upgraded.
+		Rebuild packages when build-time dependencies are upgraded.
 		"""
 
 		ebuilds = {
 			"sys-libs/x-1": { },
 			"sys-libs/x-1-r1": { },
 			"sys-libs/x-2": { },
-			"sys-apps/a-1": { "DEPEND"  : "sys-libs/x", "RDEPEND" : "sys-libs/x"},
-			"sys-apps/a-2": { "DEPEND"  : "sys-libs/x", "RDEPEND" : "sys-libs/x"},
-			"sys-apps/b-1": { "DEPEND"  : "sys-libs/x", "RDEPEND" : "sys-libs/x"},
-			"sys-apps/b-2": { "DEPEND"  : "sys-libs/x", "RDEPEND" : "sys-libs/x"},
+			"sys-apps/a-1": { "DEPEND"  : "sys-libs/x", "RDEPEND" : ""},
+			"sys-apps/a-2": { "DEPEND"  : "sys-libs/x", "RDEPEND" : ""},
+			"sys-apps/b-1": { "DEPEND"  : "sys-libs/x", "RDEPEND" : ""},
+			"sys-apps/b-2": { "DEPEND"  : "sys-libs/x", "RDEPEND" : ""},
 			"sys-apps/c-1": { "DEPEND"  : "sys-libs/x", "RDEPEND" : ""},
 			"sys-apps/c-2": { "DEPEND"  : "sys-libs/x", "RDEPEND" : ""},
 			"sys-apps/d-1": { "RDEPEND" : "sys-libs/x"},
 			"sys-apps/d-2": { "RDEPEND" : "sys-libs/x"},
-			"sys-apps/e-2": { "DEPEND"  : "sys-libs/x", "RDEPEND" : "sys-libs/x"},
-			"sys-apps/f-2": { "DEPEND"  : "sys-apps/a", "RDEPEND" : "sys-apps/a"},
+			"sys-apps/e-2": { "DEPEND"  : "sys-libs/x", "RDEPEND" : ""},
+			"sys-apps/f-2": { "DEPEND"  : "sys-apps/a", "RDEPEND" : ""},
 			"sys-apps/g-2": { "DEPEND"  : "sys-apps/b sys-libs/x",
-				"RDEPEND" : "sys-apps/b"},
+				"RDEPEND" : ""},
 			}
 
 		installed = {
 			"sys-libs/x-1": { },
-			"sys-apps/a-1": { "DEPEND"  : "sys-libs/x", "RDEPEND" : "sys-libs/x"},
-			"sys-apps/b-1": { "DEPEND"  : "sys-libs/x", "RDEPEND" : "sys-libs/x"},
+			"sys-apps/a-1": { "DEPEND"  : "sys-libs/x", "RDEPEND" : ""},
+			"sys-apps/b-1": { "DEPEND"  : "sys-libs/x", "RDEPEND" : ""},
 			"sys-apps/c-1": { "DEPEND"  : "sys-libs/x", "RDEPEND" : ""},
 			"sys-apps/d-1": { "RDEPEND" : "sys-libs/x"},
-			"sys-apps/e-1": { "DEPEND"  : "sys-libs/x", "RDEPEND" : "sys-libs/x"},
-			"sys-apps/f-1": { "DEPEND"  : "sys-apps/a", "RDEPEND" : "sys-apps/a"},
-			"sys-apps/g-1": { "DEPEND"  : "sys-apps/b sys-libs/x",
-				"RDEPEND" : "sys-apps/b"},
+			"sys-apps/e-1": { "DEPEND"  : "sys-libs/x", "RDEPEND" : ""},
+			"sys-apps/f-1": { "DEPEND"  : "sys-apps/a", "RDEPEND" : ""},
+			"sys-apps/g-1": { "DEPEND"  : "sys-apps/b",
+				"RDEPEND" : ""},
 			}
 
 		world = ["sys-apps/a", "sys-apps/b", "sys-apps/c", "sys-apps/d",
 			"sys-apps/e", "sys-apps/f", "sys-apps/g"]
 
+
 		test_cases = (
 				ResolverPlaygroundTestCase(
-					["sys-libs/x"],
+					["sys-libs/x", "sys-apps/b"],
 					options = {"--rebuild-if-unbuilt" : True,
-						"--rebuild-exclude" : ["sys-apps/b"]},
-					mergelist = ['sys-libs/x-2', 'sys-apps/a-2', 'sys-apps/e-2'],
+						"--rebuild-exclude" : ["sys-apps/c"]},
+					mergelist = ['sys-libs/x-2', 'sys-apps/a-2', 'sys-apps/b-2',
+						'sys-apps/e-2', 'sys-apps/g-2'],
 					ignore_mergelist_order = True,
 					success = True),
 
 				ResolverPlaygroundTestCase(
-					["sys-libs/x"],
+					["sys-libs/x", "sys-apps/b"],
 					options = {"--rebuild-if-unbuilt" : True},
 					mergelist = ['sys-libs/x-2', 'sys-apps/a-2', 'sys-apps/b-2',
-						'sys-apps/e-2', 'sys-apps/g-2'],
+						'sys-apps/c-2', 'sys-apps/e-2', 'sys-apps/g-2'],
 					ignore_mergelist_order = True,
 					success = True),
 
@@ -72,27 +73,29 @@ class RebuildTestCase(TestCase):
 					success = True),
 
 				ResolverPlaygroundTestCase(
-					["sys-libs/x"],
+					["sys-libs/x", "sys-apps/b"],
 					options = {"--rebuild-if-unbuilt" : True,
 						"--rebuild-ignore" : ["sys-apps/b"]},
 					mergelist = ['sys-libs/x-2', 'sys-apps/a-2', 'sys-apps/b-2',
-						'sys-apps/e-2'],
+						'sys-apps/c-2', 'sys-apps/e-2'],
 					ignore_mergelist_order = True,
 					success = True),
 
 				ResolverPlaygroundTestCase(
-					["=sys-libs/x-1-r1"],
+					["=sys-libs/x-1-r1", "sys-apps/b"],
 					options = {"--rebuild-if-unbuilt" : True},
 					mergelist = ['sys-libs/x-1-r1', 'sys-apps/a-2',
-						'sys-apps/b-2', 'sys-apps/e-2', 'sys-apps/g-2'],
+						'sys-apps/b-2', 'sys-apps/c-2', 'sys-apps/e-2',
+						'sys-apps/g-2'],
 					ignore_mergelist_order = True,
 					success = True),
 
 				ResolverPlaygroundTestCase(
-					["=sys-libs/x-1-r1"],
+					["=sys-libs/x-1-r1", "sys-apps/b"],
 					options = {"--rebuild-if-new-rev" : True},
 					mergelist = ['sys-libs/x-1-r1', 'sys-apps/a-2',
-						'sys-apps/b-2', 'sys-apps/e-2', 'sys-apps/g-2'],
+						'sys-apps/b-2', 'sys-apps/c-2', 'sys-apps/e-2',
+						'sys-apps/g-2'],
 					ignore_mergelist_order = True,
 					success = True),
 
@@ -104,10 +107,11 @@ class RebuildTestCase(TestCase):
 					success = True),
 
 				ResolverPlaygroundTestCase(
-					["sys-libs/x"],
+					["sys-libs/x", "sys-apps/b"],
 					options = {"--rebuild-if-new-ver" : True},
 					mergelist = ['sys-libs/x-2', 'sys-apps/a-2',
-						'sys-apps/b-2', 'sys-apps/e-2', 'sys-apps/g-2'],
+						'sys-apps/b-2', 'sys-apps/c-2', 'sys-apps/e-2',
+						'sys-apps/g-2'],
 					ignore_mergelist_order = True,
 					success = True),
 
@@ -119,10 +123,11 @@ class RebuildTestCase(TestCase):
 					success = True),
 
 				ResolverPlaygroundTestCase(
-					["=sys-libs/x-1"],
+					["=sys-libs/x-1", "=sys-apps/b-1"],
 					options = {"--rebuild-if-unbuilt" : True},
 					mergelist = ['sys-libs/x-1', 'sys-apps/a-2',
-						'sys-apps/b-2', 'sys-apps/e-2', 'sys-apps/g-2'],
+						'sys-apps/b-1', 'sys-apps/c-2', 'sys-apps/e-2',
+						'sys-apps/g-2'],
 					ignore_mergelist_order = True,
 					success = True),
 			)
diff --git a/pym/portage/util/env_update.py b/pym/portage/util/env_update.py
index eb8a0d9..725b0d2 100644
--- a/pym/portage/util/env_update.py
+++ b/pym/portage/util/env_update.py
@@ -19,12 +19,14 @@ from portage.process import find_binary
 from portage.util import atomic_ofstream, ensure_dirs, getconfig, \
 	normalize_path, writemsg
 from portage.util.listdir import listdir
+from portage.dbapi.vartree import vartree
+from portage.package.ebuild.config import config
 
 if sys.hexversion >= 0x3000000:
 	long = int
 
 def env_update(makelinks=1, target_root=None, prev_mtimes=None, contents=None,
-	env=None, writemsg_level=None):
+	env=None, writemsg_level=None, vardbapi=None):
 	"""
 	Parse /etc/env.d and use it to generate /etc/profile.env, csh.env,
 	ld.so.conf, and prelink.conf. Finally, run ldconfig. When ldconfig is
@@ -39,6 +41,41 @@ def env_update(makelinks=1, target_root=None, prev_mtimes=None, contents=None,
 		defaults to portage.settings["ROOT"].
 	@type target_root: String (Path)
 	"""
+	settings = getattr(portage, 'settings', None)
+	if settings is None:
+		settings = config(config_root=target_root,
+			target_root=target_root)
+
+	if 'no-env-update' in settings.features:
+		return
+
+	if vardbapi is None:
+		if isinstance(env, config):
+			vardbapi = vartree(settings=env).dbapi
+		else:
+			if target_root is None:
+				target_root = portage.settings["ROOT"]
+			if hasattr(portage, "db") and target_root in portage.db:
+				vardbapi = portage.db[target_root]["vartree"].dbapi
+			else:
+				settings = config(config_root=target_root,
+					target_root=target_root)
+				target_root = settings["ROOT"]
+				if env is None:
+					env = settings
+				vardbapi = vartree(settings=settings).dbapi
+
+	# Lock the config memory file to prevent symlink creation
+	# in merge_contents from overlapping with env-update.
+	vardbapi._fs_lock()
+	try:
+		return _env_update(makelinks, target_root, prev_mtimes, contents,
+			env, writemsg_level)
+	finally:
+		vardbapi._fs_unlock()
+
+def _env_update(makelinks, target_root, prev_mtimes, contents, env,
+	writemsg_level):
 	if writemsg_level is None:
 		writemsg_level = portage.util.writemsg_level
 	if target_root is None:
diff --git a/pym/portage/xpak.py b/pym/portage/xpak.py
index 7487d67..a939532 100644
--- a/pym/portage/xpak.py
+++ b/pym/portage/xpak.py
@@ -1,4 +1,4 @@
-# Copyright 2001-2010 Gentoo Foundation
+# Copyright 2001-2012 Gentoo Foundation
 # Distributed under the terms of the GNU General Public License v2
 
 
@@ -241,16 +241,9 @@ def getitem(myid,myitem):
 	return mydata[myloc[0]:myloc[0]+myloc[1]]
 
 def xpand(myid,mydest):
+	mydest = normalize_path(mydest) + os.sep
 	myindex=myid[0]
 	mydata=myid[1]
-	try:
-		origdir=os.getcwd()
-	except SystemExit as e:
-		raise
-	except:
-		os.chdir("/")
-		origdir="/"
-	os.chdir(mydest)
 	myindexlen=len(myindex)
 	startpos=0
 	while ((startpos+8)<myindexlen):
@@ -258,16 +251,22 @@ def xpand(myid,mydest):
 		datapos=decodeint(myindex[startpos+4+namelen:startpos+8+namelen]);
 		datalen=decodeint(myindex[startpos+8+namelen:startpos+12+namelen]);
 		myname=myindex[startpos+4:startpos+4+namelen]
-		dirname=os.path.dirname(myname)
+		myname = _unicode_decode(myname,
+			encoding=_encodings['repo.content'], errors='replace')
+		filename = os.path.join(mydest, myname.lstrip(os.sep))
+		filename = normalize_path(filename)
+		if not filename.startswith(mydest):
+			# myname contains invalid ../ component(s)
+			continue
+		dirname = os.path.dirname(filename)
 		if dirname:
 			if not os.path.exists(dirname):
 				os.makedirs(dirname)
-		mydat = open(_unicode_encode(myname,
+		mydat = open(_unicode_encode(filename,
 			encoding=_encodings['fs'], errors='strict'), 'wb')
 		mydat.write(mydata[datapos:datapos+datalen])
 		mydat.close()
 		startpos=startpos+namelen+12
-	os.chdir(origdir)
 
 class tbz2(object):
 	def __init__(self,myfile):
@@ -393,7 +392,7 @@ class tbz2(object):
 			self.datapos=a.tell()
 			a.close()
 			return 2
-		except SystemExit as e:
+		except SystemExit:
 			raise
 		except:
 			return 0
@@ -429,18 +428,11 @@ class tbz2(object):
 		"""Unpacks all the files from the dataSegment into 'mydest'."""
 		if not self.scan():
 			return 0
-		try:
-			origdir=os.getcwd()
-		except SystemExit as e:
-			raise
-		except:
-			os.chdir("/")
-			origdir="/"
+		mydest = normalize_path(mydest) + os.sep
 		a = open(_unicode_encode(self.file,
 			encoding=_encodings['fs'], errors='strict'), 'rb')
 		if not os.path.exists(mydest):
 			os.makedirs(mydest)
-		os.chdir(mydest)
 		startpos=0
 		while ((startpos+8)<self.indexsize):
 			namelen=decodeint(self.index[startpos:startpos+4])
@@ -449,18 +441,22 @@ class tbz2(object):
 			myname=self.index[startpos+4:startpos+4+namelen]
 			myname = _unicode_decode(myname,
 				encoding=_encodings['repo.content'], errors='replace')
-			dirname=os.path.dirname(myname)
+			filename = os.path.join(mydest, myname.lstrip(os.sep))
+			filename = normalize_path(filename)
+			if not filename.startswith(mydest):
+				# myname contains invalid ../ component(s)
+				continue
+			dirname = os.path.dirname(filename)
 			if dirname:
 				if not os.path.exists(dirname):
 					os.makedirs(dirname)
-			mydat = open(_unicode_encode(myname,
+			mydat = open(_unicode_encode(filename,
 				encoding=_encodings['fs'], errors='strict'), 'wb')
 			a.seek(self.datapos+datapos)
 			mydat.write(a.read(datalen))
 			mydat.close()
 			startpos=startpos+namelen+12
 		a.close()
-		os.chdir(origdir)
 		return 1
 
 	def get_data(self):
